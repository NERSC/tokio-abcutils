{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "import pandas\n",
    "import numpy\n",
    "import scipy.stats\n",
    "import abcutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Synthesize Data from CSV\n",
    "\n",
    "This process loads each summary CSV file, creates a few derived metrics, and then merges each system's CSV into a single global dataset that can be sliced and diced by system, benchmark, or any other way.  We are now caching the processed CSV in HDF5 format to speed up initial data ingest at the beginning of each analysis.  Delete the `CACHE_FILE` to re-generate this cache (e.g., when the contents of the CSV are updated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CACHE_FILE = 'cache.hdf5'\n",
    "if CACHE_FILE and os.path.isfile(CACHE_FILE):\n",
    "    print \"Loading from cache %s\" % CACHE_FILE\n",
    "    df = pandas.read_hdf(CACHE_FILE, 'summary')\n",
    "else:\n",
    "    df = pandas.concat([abcutils.load_and_synthesize_csv('summaries/edison-summaries_2017-02-14-2018-02-28.csv', system='edison'),\n",
    "                        abcutils.load_and_synthesize_csv('summaries/cori-summaries_2017-02-14-2018-02-28.csv', system='cori'),\n",
    "                        abcutils.load_and_synthesize_csv('summaries/alcf-tokio-results-2_14_17-2_15_18.csv', system='mira')],\n",
    "                       axis='rows')\n",
    "    if CACHE_FILE:\n",
    "        df.to_hdf(CACHE_FILE, key='summary', mode='w', format='fixed', complevel=9, complib='zlib')\n",
    "        print \"Cached synthesized CSV to %s\" % CACHE_FILE\n",
    "    \n",
    "# Reset the index to ensure that there are no degenerate indices in the final dataframe\n",
    "df.index = pandas.Index(data=numpy.arange(len(df)), dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd2epoch = lambda x: time.mktime(x.to_pydatetime().timetuple())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by = ['_test_platform', '_benchmark_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate a Single Test Platform\n",
    "\n",
    "Look at one combination of (compute system, file system, benchmark) to show what this UMAMI analysis can do.\n",
    "\n",
    "### Define Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_platform = 'scratch2@edison'\n",
    "test_platform = 'cscratch@cori-knl'\n",
    "#test_platform = 'cscratch@cori-haswell'\n",
    "# test_platform = 'mira-fs1@mira'\n",
    "\n",
    "benchmark_id = 'ior_fpp_write'\n",
    "# benchmark_id = 'dbscan_read_shared_read'\n",
    "# benchmark_id = 'vpicio_uni_shared_write'\n",
    "# benchmark_id = 'ior_shared_write'\n",
    "# benchmark_id = 'hacc_io_read_fpp_read'\n",
    "\n",
    "plot_metric = 'darshan_agg_perf_by_slowest_posix_gibs'\n",
    "date_start = datetime.datetime(2017, 2, 14)\n",
    "date_end = datetime.datetime(2018, 3, 1)\n",
    "\n",
    "group_by = ['_test_platform', '_benchmark_id']\n",
    "delta = datetime.timedelta(days=1).total_seconds()\n",
    "\n",
    "filtered_df = df.groupby(by=group_by).get_group((test_platform, benchmark_id))\n",
    "filtered_df = filtered_df[filtered_df['darshan_total_gibs_posix'] > 1.0]\n",
    "filtered_df = filtered_df[filtered_df['_datetime_start'] < date_end]\n",
    "filtered_df = filtered_df[filtered_df['_datetime_start'] >= date_start]\n",
    "filtered_df = filtered_df[filtered_df['_benchmark_id'] != 'hacc_io_write_shared_write']\n",
    "\n",
    "print \"test_platform =\", test_platform\n",
    "print \"benchmark_id =\", abcutils.CONFIG['benchmark_labels'].get(benchmark_id, benchmark_id)\n",
    "print \"plot_metric =\", abcutils.CONFIG['metric_labels'].get(plot_metric, plot_metric)\n",
    "print \"date_start =\", date_start.isoformat()\n",
    "print \"date_end =\", date_end.isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Width of simple moving average (SMA) short/long windows\n",
    "short_window = 7\n",
    "long_window = 28\n",
    "\n",
    "print \"Short window will average over %d measurements at a time\" % short_window\n",
    "print \"Long window will average over %d measurements at a time\" % long_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics to include in UMAMI renderings and analysis.  Anything that\n",
    "# _might_ affect performance should be included here.\n",
    "umami_rows = [\n",
    "    'darshan_agg_perf_by_slowest_posix_gibs',\n",
    "    'coverage_factor_bw',\n",
    "    'coverage_factor_nodehrs',\n",
    "    'fs_ave_mds_cpu',\n",
    "    'fs_tot_metadata_ops',\n",
    "    'fs_ave_oss_cpu',\n",
    "    'fs_tot_open_ops',\n",
    "    'fshealth_ost_most_full_pct',\n",
    "    'fshealth_ost_overloaded_oss_count',\n",
    "    'jobsdb_concurrent_nodes',\n",
    "    'topology_job_max_radius',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Simple Moving Averages (SMAs)\n",
    "\n",
    "Compare a short-window SMA and a long-window SMA and use the places where they cross over to divide the entire year into _regions_ of interesting benchmark behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each region defined above, find the _minimum_ performance observed and denote that measurement (and its associated job) as a _locus_.  We then collate all _loci_ into a set of poorly performing benchmarks that are worth contextualizing with UMAMI.\n",
    "\n",
    "We also plot the raw performance data (light blue bars), the short SMA (orange line), the long SMA (green line), and all loci (red bars) to visually verify that the loci we've identified are indeed poorly performing jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_raw = filtered_df['_datetime_start'].apply(lambda x: time.mktime(x.timetuple()))\n",
    "y_raw = filtered_df[plot_metric]\n",
    "\n",
    "loci = abcutils.features.generate_loci_sma(filtered_df, plot_metric, mins=True, maxes=False)\n",
    "y_low = filtered_df.loc[loci.index][plot_metric]\n",
    "x_low = [pd2epoch(x) for x in loci['_datetime_start']]\n",
    "print \"Found %d loci across %s\" % (len(loci), date_end - date_start)\n",
    "\n",
    "### plot the raw data\n",
    "fig, ax = matplotlib.pyplot.subplots()\n",
    "fig.set_size_inches(16, 4)\n",
    "ax.grid()\n",
    "ax.bar(x_raw, y_raw, width=delta, alpha=0.5)\n",
    "ax.bar(x_low, y_low, width=delta, color='red', alpha=0.5)\n",
    "ax.set_ylabel(abcutils.CONFIG['metric_labels'].get(plot_metric, plot_metric).replace(\" \", \"\\n\"))\n",
    "ax.set_xticklabels([datetime.datetime.fromtimestamp(x).strftime(\"%b %d\") for x in ax.get_xticks()])\n",
    "fig.suptitle(\"%s on %s\" % (abcutils.CONFIG['benchmark_labels'].get(benchmark_id, benchmark_id),\n",
    "                           test_platform))\n",
    "\n",
    "### also calculate and plot the SMAs\n",
    "sma_short = abcutils.features.calculate_sma(filtered_df, '_datetime_start', plot_metric, short_window)\n",
    "sma_long = abcutils.features.calculate_sma(filtered_df, '_datetime_start', plot_metric, long_window)\n",
    "\n",
    "### plot the intercept points demarcating different regions\n",
    "# sma_intercepts = abcutils.features.sma_intercepts(filtered_df, plot_metric, short_window, long_window)\n",
    "#intercepts = numpy.array([(filtered_df['_datetime_start'].loc[x], filtered_df[plot_metric].loc[x]) for x in sma_intercepts.index])\n",
    "#x_intercept = [(x - numpy.datetime64('1970-01-01T00:00:00Z')) / numpy.timedelta64(1, 's') for x in intercepts[:, 0]]\n",
    "#y_intercept = intercepts[:, 1]\n",
    "#ax.scatter(x_intercept, y_intercept, color='red', marker='.')\n",
    "\n",
    "x_sma_short = [pd2epoch(x) for x in sma_short.index]\n",
    "y_sma_short = sma_short.values\n",
    "\n",
    "x_sma_long = [pd2epoch(x) for x in sma_long.index]\n",
    "y_sma_long = sma_long.values\n",
    "\n",
    "ax.plot(x_sma_short, y_sma_short, color='C1', linewidth=2)\n",
    "ax.plot(x_sma_long, y_sma_long, color='C2', linewidth=2)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate UMAMI Diagrams Around Loci\n",
    "\n",
    "Generate UMAMI diagrams that _end_ at each locus and have `long_window` days' of benchmark data preceding them.  Don't bother creating UMAMI diagrams for benchmarks with fewer than `short_window` benchmark data in the preceding `long_window` days.\n",
    "\n",
    "Note that this process mixes up the semantic meaning of `long_window`.  When defining loci, `long_window` refers to a number of benchmark measurements, not days.  Ideally, one benchmark runs each day so this semantic difference is trivial.  However in reality, there are days when no benchmarks are run meaning loci are defined using a series of `long_window` benchmark measurements that often span _more than_ `long_window` days.\n",
    "\n",
    "Practically speaking, this does not change very much as long as the ratio of `long_window` in days to `long_window` in benchmark measurements is close to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_renders = 1\n",
    "\n",
    "print \"Rendering a maximum of %d UMAMI diagrams\" % max_renders\n",
    "\n",
    "rendered = 0\n",
    "for locus in loci.itertuples():\n",
    "    region_idx0 = filtered_df.index.get_loc(locus.region_start)\n",
    "    region_idxf = filtered_df.index.get_loc(locus.region_end)\n",
    "    umami_region = filtered_df.iloc[region_idx0:region_idxf]\n",
    "    print locus\n",
    "\n",
    "    if len(umami_region) >= short_window:\n",
    "        abcutils.plot.generate_umami(umami_region, umami_rows, highlight_index=umami_region.index.get_loc(locus.Index))\n",
    "        rendered += 1\n",
    "        if rendered == max_renders:\n",
    "            break\n",
    "    else:\n",
    "        print \"Skipping locus at %s because it has only %d data points (%d required)\" % (umami_region['_datetime_start'], len(umami_region), short_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabulate frequency of different problems\n",
    "\n",
    "Because we define loci to be local minima (i.e., the worst benchmark measured in a temporally local region), all of the UMAMIs we generated above _should_ end on an extremely bad day.  To automatically identify the possible causes for bad performance at each locus, we look at all of the UMAMI metrics and flag those that also ended on extremely poor (e.g., worst quartile) values.  This is exactly the same process we used in the PDSW'17 paper's case studies, but now we have automated the process.\n",
    "\n",
    "With this method of flagging, we keep a running total of metrics that were flagged as possible culprits as we examine each locus.  Note that multiple metrics can be flagged for a single locus (e.g., low coverage factor _and_ high MDS load can both be flagged for a single benchmark run), so the sum of flags over all metrics will usually add up to more than the total number of loci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_contributors(dataframe, dependent_column, expected_minima=-1, want_good=False):\n",
    "    \"\"\"Identify secondary metrics that coincide with a good/bad primary metric\n",
    "    \n",
    "    Args:\n",
    "        dataframe (DataFrame): dataframe containing one metric per column\n",
    "            over a series of measurements\n",
    "        dependent_column (str): name of column in `dataframe` corresponding\n",
    "            to the metric to which contributors will be identified\n",
    "        want_good (bool): are we identifying metrics that are unusually good\n",
    "            (True) or unusually bad (False)?\n",
    "        expected_minima (int): iloc of `dataframe` that is the expected local\n",
    "            minima; default of -1 selects the final value in the dataframe\n",
    "    Returns:\n",
    "        List of dicts, where each dicts corresponds to a single metric that\n",
    "        was identified as meeting the contribution criteria.  A dict\n",
    "        contains the 'error' key and a True value when `dependent_column` does\n",
    "        not fall within the most extreme quartile.\n",
    "    \"\"\"\n",
    "    contributors = []\n",
    "    for column in dataframe.columns:\n",
    "        big_is_good = abcutils.CONFIG['metric_big_is_good'].get(column, True)\n",
    "\n",
    "        result = None\n",
    "        # we want the value to be lower than the cutoff when either\n",
    "        # (a) we're looking for bad, and big IS good, or\n",
    "        # (b) we're looking for good, and big IS NOT good\n",
    "        if (not want_good and big_is_good) or (want_good and not big_is_good):\n",
    "            try:\n",
    "                cutoff = numpy.nanpercentile(dataframe[column].iloc[0:-1], 25)\n",
    "            except TypeError: # if passed non-numeric columns, just skip them\n",
    "                continue\n",
    "            if dataframe[column].iloc[expected_minima] < cutoff:\n",
    "                result = {\n",
    "                    'metric': column,\n",
    "                    'value': dataframe[column].iloc[expected_minima],\n",
    "                    'comparator': \"<\",\n",
    "                    'cutoff': cutoff,\n",
    "                }\n",
    "        # we want the value to be higher than the cutoff when either\n",
    "        # (a) we're looking for good, and big IS good\n",
    "        # (b) we're looking for bad, and big IS NOT good\n",
    "        elif (want_good and big_is_good) or (not want_good and not big_is_good):\n",
    "            try:\n",
    "                cutoff = numpy.nanpercentile(dataframe[column].iloc[0:-1], 75)\n",
    "            except TypeError:\n",
    "                continue\n",
    "            if dataframe[column].iloc[expected_minima] > cutoff:\n",
    "                result = {\n",
    "                    'metric': column,\n",
    "                    'value': dataframe[column].iloc[-1],\n",
    "                    'comparator': \">\",\n",
    "                    'cutoff': cutoff,\n",
    "                }\n",
    "        \n",
    "        if column == dependent_column:\n",
    "            if result is None and expected_minima is not None:\n",
    "                warnings.warn(\"%s=%s (index %s) not in the %s quartile (%s) of %d values\" % \n",
    "                              (column,\n",
    "                               dataframe[column].iloc[expected_minima],\n",
    "                               dataframe[column].index[expected_minima],\n",
    "                               \"best\" if want_good else \"worst\",\n",
    "                               cutoff,\n",
    "                               len(dataframe)))\n",
    "                print dataframe[['_datetime_start', column]]\n",
    "                raise Exception\n",
    "                return [{\"error\": True}]\n",
    "        elif result:\n",
    "            contributors.append(result)\n",
    "                \n",
    "    return contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_contributors(dataframe, plot_metric, loci, window_days, min_points, want_good=False):\n",
    "    \"\"\"Count the secondary metrics that may have contributed to good/bad primary metric\n",
    "    \n",
    "    Args:\n",
    "        dataframe (DataFrame): dataframe containing one metric per column\n",
    "            over a series of measurements\n",
    "        plot_metric (str): name of column corresponding to the primary metric\n",
    "            of performance\n",
    "        loci (list of datetime): datetimes denoting the end of a region of\n",
    "            interest over which the UMAMI analysis should be conducted\n",
    "        window_days (int): how many days prior to each loci to use when\n",
    "            determining the UMAMI percentiles\n",
    "        min_points (int): the minimum number of measurements that must fall\n",
    "            within each locus and (locus - window_days) for the contributors\n",
    "            to be counted\n",
    "        want_good (bool): are we identifying metrics that are unusually good\n",
    "            (True) or unusually bad (False)?\n",
    "\n",
    "    Returns:\n",
    "        Dict keyed by the columns of `dataframe` and whose values are the number\n",
    "        of times each key was identified as a contributor to extreme performance.\n",
    "        Also includes the following special keys:\n",
    "            * `_loci_ignored`: number of loci not examined due to the window\n",
    "              containing fewer than `min_points` benchmark measurements\n",
    "            * `_loci_unclassified`: number of loci which had no contributors\n",
    "            * `_loci_classified`: number of loci for which contributors were found\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        '_loci_ignored': 0,\n",
    "        '_loci_unclassified': 0,\n",
    "        '_loci_classified': 0,\n",
    "        '_loci_errors': 0,\n",
    "    }\n",
    "    abcutils.CONFIG['metric_labels']['_loci_unclassified'] = \"Indeterminate\"\n",
    "    for locus in loci.itertuples():\n",
    "        region_idx0 = dataframe.index.get_loc(locus.region_start)\n",
    "        region_idxf = dataframe.index.get_loc(locus.region_end)\n",
    "        region_df = dataframe.iloc[region_idx0:region_idxf]\n",
    "        expected_minima = region_df.index.get_loc(locus.Index)\n",
    "        if len(region_df) < min_points:\n",
    "            results['_loci_ignored'] += 1\n",
    "        else:\n",
    "            contributors = identify_contributors(region_df,\n",
    "                                                 plot_metric,\n",
    "                                                 want_good=want_good,\n",
    "                                                 expected_minima=expected_minima)\n",
    "            if len(contributors) == 0:\n",
    "                results['_loci_unclassified'] += 1\n",
    "            elif len(contributors) == 1 and contributors[0].get('error', False):\n",
    "                results['_loci_errors'] += 1\n",
    "            else:\n",
    "                results['_loci_classified'] += 1\n",
    "                for contributor in contributors:\n",
    "                    results[contributor['metric']] = results.get(contributor['metric'], 0) + 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loci = abcutils.features.generate_loci_sma(filtered_df, plot_metric, mins=True, maxes=False)\n",
    "\n",
    "results = count_contributors(dataframe=filtered_df[['_datetime_start'] + umami_rows],\n",
    "                             plot_metric=plot_metric,\n",
    "                             loci=loci,\n",
    "                             window_days=long_window,\n",
    "                             min_points=short_window,\n",
    "                             want_good=False)\n",
    "\n",
    "num_classified = results.pop('_loci_classified')\n",
    "num_unclassified = results.pop('_loci_unclassified')\n",
    "num_ignored = results.pop('_loci_ignored')\n",
    "num_errors = results.pop('_loci_errors')\n",
    "\n",
    "print \"Classified: %d\" % num_classified\n",
    "print \"Unclassified: %d\" % num_unclassified\n",
    "print \"Ignored: %d\" % num_ignored\n",
    "print \"Errors: %d\" % num_errors\n",
    "print\n",
    "print \"Number of times each metric was flagged across %d loci:\" % (len(loci))\n",
    "for key in reversed(sorted(results.keys(), key=lambda x: results[x])):\n",
    "    print \"%3d %s\" % (results[key], abcutils.CONFIG['metric_labels'].get(key, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabulate contributors to bad performance over all tests\n",
    "\n",
    "We now apply the above analysis to the entirety of the benchmark data across all systems.\n",
    "\n",
    "Note that warnings about certain loci not being in the worst quartile indicate that the SMA-based method we use to identify local minima is not perfect.  There are a variety of other methods (including some canned algorithms) that we can swap in to improve our classification of loci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Global plot parameters\n",
    "plot_metric = 'darshan_agg_perf_by_slowest_posix_gibs'\n",
    "# date_start = datetime.datetime(2017, 2, 14)\n",
    "# date_end = datetime.datetime(2018, 3, 1)\n",
    "group_by = ['_test_platform', '_benchmark_id']\n",
    "\n",
    "# Determine which plots to generate\n",
    "test_platforms = sorted(df['_test_platform'].unique())\n",
    "benchmark_ids = sorted(df[df['_benchmark_id'] != 'hacc_io_write_shared_write']['_benchmark_id'].unique())\n",
    "\n",
    "# test_platforms = ['cscratch@cori-knl']\n",
    "# benchmark_ids = ['dbscan_read_shared_read', 'vpicio_uni_shared_write']\n",
    "\n",
    "print \"plot_metric =\", abcutils.CONFIG['metric_labels'].get(plot_metric, plot_metric)\n",
    "print \"date_start =\", date_start.isoformat()\n",
    "print \"date_end =\", date_end.isoformat()\n",
    "\n",
    "grouped_df = df.groupby(by=group_by)\n",
    "\n",
    "results_flat = {}\n",
    "results = []\n",
    "for test_platform in test_platforms:\n",
    "    for benchmark_id in benchmark_ids:\n",
    "        try:\n",
    "            filtered_df = grouped_df.get_group((test_platform, benchmark_id))\n",
    "        except KeyError:\n",
    "            continue\n",
    "        filtered_df = filtered_df[filtered_df['darshan_total_gibs_posix'] > 1.0]\n",
    "        filtered_df = filtered_df[filtered_df['_datetime_start'] < date_end]\n",
    "        filtered_df = filtered_df[filtered_df['_datetime_start'] >= date_start]\n",
    "\n",
    "        loci = abcutils.features.generate_loci_sma(filtered_df, plot_metric, mins=True, maxes=False)\n",
    "        result = count_contributors(dataframe=filtered_df[['_datetime_start'] + umami_rows],\n",
    "                                    plot_metric=plot_metric,\n",
    "                                    loci=loci,\n",
    "                                    window_days=long_window,\n",
    "                                    min_points=short_window,\n",
    "                                    want_good=False)\n",
    "        for key, value in result.iteritems():\n",
    "            results_flat[key] = results_flat.get(key, 0) + value\n",
    "        result['_test_platform'] = test_platform\n",
    "        result['_benchmark_id'] = benchmark_id\n",
    "        results.append(result)\n",
    "\n",
    "num_classified = results_flat.pop('_loci_classified')\n",
    "num_unclassified = results_flat.get('_loci_unclassified')\n",
    "num_ignored = results_flat.pop('_loci_ignored')\n",
    "num_errors = results_flat.pop('_loci_errors')\n",
    "num_loci = num_classified + num_unclassified + num_ignored + num_errors\n",
    "\n",
    "print \"Classified: %d\" % num_classified\n",
    "print \"Unclassified: %d\" % num_unclassified\n",
    "print \"Ignored: %d\" % num_ignored\n",
    "print \"Errors: %d\" % num_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following bar graph shows the total number of times each metric has been flagged as a possible source of performance loss as defined above: its value was \"bad\" coincident with each locus, where a locus is a job whose performance was abnormally poor and \"bad\" is defined as being within the 25th worst percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results into a DataFrame that we can slice and dice\n",
    "results_df = pandas.DataFrame.from_dict(results, orient='columns')\n",
    "grouped_df = results_df.groupby(by='_benchmark_id')\n",
    "\n",
    "# Sort metric order by its impact\n",
    "x_labels = [x for x in results_df.columns if (not x.startswith('_') or x == '_loci_unclassified')]\n",
    "x_sums = list(enumerate([results_df[x].sum() for x in x_labels]))\n",
    "new_metric_order = [xx[0] for xx in sorted(x_sums, key=lambda x:x[1], reverse=True)]\n",
    "x_labels = [x_labels[i] for i in new_metric_order]\n",
    "\n",
    "# Create plot canvas\n",
    "fig, ax = matplotlib.pyplot.subplots()\n",
    "fig.set_size_inches(8,4)\n",
    "\n",
    "\n",
    "x_values = numpy.arange(len(y_values))\n",
    "y_bottom = numpy.zeros(len(x_labels))\n",
    "for _benchmark_id in benchmark_ids:\n",
    "    try:\n",
    "        filtered_df = grouped_df.get_group((_benchmark_id))\n",
    "    except KeyError:\n",
    "        continue\n",
    "    y_values = numpy.array([filtered_df[x].sum() for x in x_labels])\n",
    "    ax.bar(x=x_values, height=y_values, bottom=y_bottom, width=0.9, label=abcutils.CONFIG['benchmark_labels_short'].get(_benchmark_id, _benchmark_id))\n",
    "    y_bottom += y_values\n",
    "\n",
    "ax.yaxis.grid(True)        \n",
    "ax.set_xticks(x_values)\n",
    "ax.set_xticklabels([abcutils.CONFIG['metric_labels'].get(x, x) for x in x_labels], rotation=30, ha='right')\n",
    "ax.set_ylabel(\"Number of occurrences\")\n",
    "ax.set_title(\"Candidate Contributors to Good Performance (%d Jobs Total)\" % num_loci)\n",
    "ax.legend(bbox_to_anchor=(1.0, 1.00))\n",
    "\n",
    "for index, x_value in enumerate(x_values):\n",
    "    ax.annotate(\"%d\" % y_bottom[index], xy=(x_value, y_bottom[index]), ha='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = matplotlib.pyplot.subplots()\n",
    "fig.set_size_inches(16,4)\n",
    "\n",
    "x_labels = list(reversed(sorted(results_flat.keys(), key=lambda x: results_flat[x])))\n",
    "y_values = [results_flat[x] for x in x_labels]\n",
    "x_values = numpy.arange(len(y_values))\n",
    "\n",
    "ax.yaxis.grid(True)\n",
    "ax.bar(x_values, y_values)\n",
    "ax.set_xticks(x_values)\n",
    "ax.set_xticklabels([abcutils.CONFIG['metric_labels'].get(x, x) for x in x_labels], rotation=45, ha='right')\n",
    "ax.set_ylabel(\"Number of occurrences\")\n",
    "ax.set_title(\"Candidate Contributors to Bad Performance (%d Jobs Total)\" % num_loci)\n",
    "\n",
    "for index, x_value in enumerate(x_values):\n",
    "    ax.annotate(\"%d\" % y_values[index], xy=(x_value, y_values[index]), ha='center')\n",
    "            \n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many caveats with the above plot; notably, the majority of jobs were run on Lustre since this data includes all Edison, Cori+KNL, and Cori+Haswell jobs.  In addition, the Mira data does not currently contain file system health data (although it is available), so the \"Number of Overloaded OSSes\" may be underreported.\n",
    "\n",
    "The most appropriate way to present this data is to produce one bar graph per test platform (compute system + file system combination) so that metrics that are only available on one test platform aren't being directly compared with others that are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabulate contributors to good performance over all tests\n",
    "\n",
    "We can also perform the above analysis and look for the metrics that coincided with good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_raw = filtered_df['_datetime_start'].apply(lambda x: time.mktime(x.timetuple()))\n",
    "y_raw = filtered_df[plot_metric]\n",
    "\n",
    "loci = abcutils.features.generate_loci_sma(filtered_df, plot_metric, mins=False, maxes=True)\n",
    "y_low = filtered_df.loc[loci.index][plot_metric]\n",
    "x_low = [pd2epoch(x) for x in loci['_datetime_start']]\n",
    "print \"Found %d loci across %s\" % (len(loci), date_end - date_start)\n",
    "\n",
    "### plot the raw data\n",
    "fig, ax = matplotlib.pyplot.subplots()\n",
    "fig.set_size_inches(16, 4)\n",
    "ax.grid()\n",
    "ax.bar(x_raw, y_raw, width=delta, alpha=0.5)\n",
    "ax.bar(x_low, y_low, width=delta, color='red')\n",
    "ax.set_ylabel(abcutils.CONFIG['metric_labels'].get(plot_metric, plot_metric).replace(\" \", \"\\n\"))\n",
    "ax.set_xticklabels([datetime.datetime.fromtimestamp(x).strftime(\"%b %d\") for x in ax.get_xticks()])\n",
    "fig.suptitle(\"%s on %s\" % (abcutils.CONFIG['benchmark_labels'].get(benchmark_id, benchmark_id),\n",
    "                           test_platform))\n",
    "\n",
    "### also calculate and plot the SMAs\n",
    "sma_short = abcutils.features.calculate_sma(filtered_df, '_datetime_start', plot_metric, short_window)\n",
    "sma_long = abcutils.features.calculate_sma(filtered_df, '_datetime_start', plot_metric, long_window)\n",
    "\n",
    "### plot the intercept points demarcating different regions\n",
    "#sma_intercepts = abcutils.features.sma_intercepts(filtered_df, plot_metric, short_window, long_window)\n",
    "#intercepts = numpy.array([(filtered_df['_datetime_start'].loc[x], filtered_df[plot_metric].loc[x]) for x in sma_intercepts.index])\n",
    "#x_intercept = [(x - numpy.datetime64('1970-01-01T00:00:00Z')) / numpy.timedelta64(1, 's') for x in intercepts[:, 0]]\n",
    "#y_intercept = intercepts[:, 1]\n",
    "#ax.scatter(x_intercept, y_intercept, color='red', marker='.')\n",
    "\n",
    "x_sma_short = [pd2epoch(x) for x in sma_short.index]\n",
    "y_sma_short = sma_short.values\n",
    "\n",
    "x_sma_long = [pd2epoch(x) for x in sma_long.index]\n",
    "y_sma_long = sma_long.values\n",
    "\n",
    "ax.plot(x_sma_short, y_sma_short, color='C1', linewidth=2)\n",
    "ax.plot(x_sma_long, y_sma_long, color='C2', linewidth=2)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print \"plot_metric =\", abcutils.CONFIG['metric_labels'].get(plot_metric, plot_metric)\n",
    "print \"date_start =\", date_start.isoformat()\n",
    "print \"date_end =\", date_end.isoformat()\n",
    "\n",
    "errors = 0\n",
    "results_flat = {}\n",
    "results = []\n",
    "for test_platform in test_platforms:\n",
    "    for benchmark_id in benchmark_ids:\n",
    "        try:\n",
    "            filtered_df = grouped_df.get_group((test_platform, benchmark_id))\n",
    "        except KeyError:\n",
    "            continue\n",
    "        filtered_df = filtered_df[filtered_df['darshan_total_gibs_posix'] > 1.0]\n",
    "        filtered_df = filtered_df[filtered_df['_datetime_start'] < date_end]\n",
    "        filtered_df = filtered_df[filtered_df['_datetime_start'] >= date_start]\n",
    "\n",
    "        loci = abcutils.features.generate_loci_sma(filtered_df, plot_metric, mins=False, maxes=True)\n",
    "        result = count_contributors(dataframe=filtered_df[['_datetime_start'] + umami_rows],\n",
    "                                    plot_metric=plot_metric,\n",
    "                                    loci=loci,\n",
    "                                    window_days=long_window,\n",
    "                                    min_points=short_window,\n",
    "                                    want_good=True)\n",
    "        for key, value in result.iteritems():\n",
    "            results_flat[key] = results_flat.get(key, 0) + value\n",
    "        result['_test_platform'] = test_platform\n",
    "        result['_benchmark_id'] = benchmark_id\n",
    "        results.append(result)\n",
    "\n",
    "num_classified = results_flat.pop('_loci_classified')\n",
    "num_unclassified = results_flat.get('_loci_unclassified')\n",
    "num_ignored = results_flat.pop('_loci_ignored')\n",
    "num_errors = results_flat.pop('_loci_errors')\n",
    "num_loci = num_classified + num_unclassified + num_ignored + num_errors\n",
    "\n",
    "print \"Classified: %d\" % num_classified\n",
    "print \"Unclassified: %d\" % num_unclassified\n",
    "print \"Ignored: %d\" % num_ignored\n",
    "print \"Errors: %d\" % num_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results into a DataFrame that we can slice and dice\n",
    "results_df = pandas.DataFrame.from_dict(results, orient='columns')\n",
    "grouped_df = results_df.groupby(by='_benchmark_id')\n",
    "\n",
    "# Sort metric order by its impact\n",
    "x_labels = [x for x in results_df.columns if (not x.startswith('_') or x == '_loci_unclassified')]\n",
    "x_sums = list(enumerate([results_df[x].sum() for x in x_labels]))\n",
    "new_metric_order = [xx[0] for xx in sorted(x_sums, key=lambda x:x[1], reverse=True)]\n",
    "x_labels = [x_labels[i] for i in new_metric_order]\n",
    "\n",
    "# Create plot canvas\n",
    "fig, ax = matplotlib.pyplot.subplots()\n",
    "fig.set_size_inches(8,4)\n",
    "\n",
    "\n",
    "x_values = numpy.arange(len(y_values))\n",
    "y_bottom = numpy.zeros(len(x_labels))\n",
    "for _benchmark_id in benchmark_ids:\n",
    "    try:\n",
    "        filtered_df = grouped_df.get_group((_benchmark_id))\n",
    "    except KeyError:\n",
    "        continue\n",
    "    y_values = numpy.array([filtered_df[x].sum() for x in x_labels])\n",
    "    ax.bar(x=x_values, height=y_values, bottom=y_bottom, width=0.9, label=abcutils.CONFIG['benchmark_labels_short'].get(_benchmark_id, _benchmark_id))\n",
    "    y_bottom += y_values\n",
    "\n",
    "ax.yaxis.grid(True)        \n",
    "ax.set_xticks(x_values)\n",
    "ax.set_xticklabels([abcutils.CONFIG['metric_labels'].get(x, x) for x in x_labels], rotation=30, ha='right')\n",
    "ax.set_ylabel(\"Number of occurrences\")\n",
    "ax.set_title(\"Candidate Contributors to Good Performance (%d Jobs Total)\" % num_loci)\n",
    "ax.legend(bbox_to_anchor=(1.0, 1.00))\n",
    "\n",
    "for index, x_value in enumerate(x_values):\n",
    "    ax.annotate(\"%d\" % y_bottom[index], xy=(x_value, y_bottom[index]), ha='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = matplotlib.pyplot.subplots()\n",
    "fig.set_size_inches(16,4)\n",
    "\n",
    "x_labels = list(reversed(sorted(results_flat.keys(), key=lambda x: results_flat[x])))\n",
    "y_values = [results_flat[x] for x in x_labels]\n",
    "x_values = numpy.arange(len(y_values))\n",
    "ax.yaxis.grid(True)\n",
    "ax.bar(x_values, y_values)\n",
    "ax.set_xticks(x_values)\n",
    "ax.set_xticklabels([abcutils.CONFIG['metric_labels'].get(x, x) for x in x_labels], rotation=45, ha='right')\n",
    "ax.set_ylabel(\"Number of occurrences\")\n",
    "ax.set_title(\"Candidate Contributors to Good Performance (%d Jobs Total)\" % num_loci)\n",
    "\n",
    "for index, x_value in enumerate(x_values):\n",
    "    ax.annotate(\"%d\" % y_values[index], xy=(x_value, y_values[index]), ha='center')\n",
    "            \n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
