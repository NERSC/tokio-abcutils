{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "import pandas\n",
    "import numpy\n",
    "import scipy.stats\n",
    "import abcutils\n",
    "\n",
    "numpy.random.seed(int(time.mktime(datetime.datetime.now().timetuple())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Synthesize Data from CSV\n",
    "\n",
    "This process loads each summary CSV file, creates a few derived metrics, and then merges each system's CSV into a single global dataset that can be sliced and diced by system, benchmark, or any other way.  We are now caching the processed CSV in HDF5 format to speed up initial data ingest at the beginning of each analysis.  Delete the `CACHE_FILE` to re-generate this cache (e.g., when the contents of the CSV are updated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_df = abcutils.sc18paper.load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate a Single Test Platform\n",
    "\n",
    "Look at one combination of (compute system, file system, benchmark) to show what this UMAMI analysis can do.\n",
    "\n",
    "### Define Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TEST_PLATFORM = 'scratch2@edison'\n",
    "# TEST_PLATFORM = 'cscratch@cori-knl'\n",
    "# TEST_PLATFORM = 'cscratch@cori-haswell'\n",
    "TEST_PLATFORM = 'mira-fs1@mira'\n",
    "\n",
    "# BENCHMARK_ID = 'ior_fpp_write'\n",
    "BENCHMARK_ID = 'dbscan_read_shared_read'\n",
    "# BENCHMARK_ID = 'vpicio_uni_shared_write'\n",
    "# BENCHMARK_ID = 'ior_shared_write'\n",
    "# BENCHMARK_ID = 'hacc_io_read_fpp_read'\n",
    "\n",
    "plot_metric = 'darshan_normalized_perf_by_max'\n",
    "delta = datetime.timedelta(days=1).total_seconds()\n",
    "\n",
    "group_by = ['_test_platform', '_benchmark_id']\n",
    "\n",
    "filtered_df['random'] = numpy.random.random((len(filtered_df), ))\n",
    "\n",
    "example_df = filtered_df.groupby(by=group_by).get_group((TEST_PLATFORM, BENCHMARK_ID)).copy()\n",
    "\n",
    "print \"test_platform =\", TEST_PLATFORM\n",
    "print \"benchmark_id =\", abcutils.CONFIG['benchmark_labels'].get(BENCHMARK_ID, BENCHMARK_ID)\n",
    "print \"plot_metric =\", abcutils.CONFIG['metric_labels'].get(plot_metric, plot_metric)\n",
    "print \"date_start =\", abcutils.sc18paper.DATE_START.isoformat()\n",
    "print \"date_end =\", abcutils.sc18paper.DATE_END.isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Width of simple moving average (SMA) short/long windows\n",
    "short_window = abcutils.features.SHORT_WINDOW\n",
    "long_window = abcutils.features.LONG_WINDOW\n",
    "short_window = pandas.Timedelta(days=14)\n",
    "long_window = pandas.Timedelta(days=49)\n",
    "\n",
    "min_measures = abcutils.features.MIN_REGION\n",
    "\n",
    "print \"Short window will average over %s measurements at a time\" % short_window\n",
    "print \"Long window will average over %s measurements at a time\" % long_window\n",
    "print \"Ignoring regions with fewer than %d measurements\" % min_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics to include in UMAMI renderings and analysis.  Anything that\n",
    "# _might_ affect performance should be included here.\n",
    "umami_rows = {\n",
    "    'darshan_normalized_perf_by_max': \"Application Performance\",\n",
    "    'coverage_factor_bw': \"Bandwidth CF\",\n",
    "#   'coverage_factor_nodehrs',\n",
    "    'coverage_factor_opens': \"open(2) CF\",\n",
    "    'coverage_factor_stats': \"stat(2) CF\",\n",
    "    'coverage_factor_ops': \"IOPS CF\",\n",
    "    'fs_ave_mds_cpu': \"Metadata Server Load\",\n",
    "#   'fs_tot_metadata_ops',\n",
    "    'fs_ave_oss_cpu': \"Data Server Load\",\n",
    "#   'fs_tot_open_ops',\n",
    "    'fshealth_ost_most_full_pct': \"File System Fullness\",\n",
    "    'fshealth_ost_overloaded_oss_count': \"LUN Failover Count\",\n",
    "#   'jobsdb_concurrent_nodes',\n",
    "    'topology_job_max_radius': \"Max Job Radius\",\n",
    "    'random': \"Random Variable\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Simple Moving Averages (SMAs)\n",
    "\n",
    "Compare a short-window SMA and a long-window SMA and use the places where they cross over to divide the entire year into _regions_ of interesting benchmark behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each region defined above, find the _minimum_ performance observed and denote that measurement (and its associated job) as a _locus_.  We then collate all _loci_ into a set of poorly performing benchmarks that are worth contextualizing with UMAMI.\n",
    "\n",
    "We also plot the raw performance data (light blue bars), the short SMA (orange line), the long SMA (green line), and all loci (red bars) to visually verify that the loci we've identified are indeed poorly performing jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "intercepts = abcutils.features.sma_intercepts(example_df, plot_metric, short_window=short_window, long_window=long_window)\n",
    "loci = abcutils.features.generate_loci_sma(example_df,\n",
    "                                           plot_metric,\n",
    "                                           mins=True,\n",
    "                                           maxes=False,\n",
    "                                           short_window=short_window,\n",
    "                                           long_window=long_window)\n",
    "\n",
    "ax = abcutils.plot.locus_summary(example_df, plot_metric, loci, align=\"edge\")\n",
    "ax.get_figure().suptitle(\"%s on %s\" % (abcutils.CONFIG['benchmark_labels'].get(BENCHMARK_ID, BENCHMARK_ID), TEST_PLATFORM))\n",
    "\n",
    "# Add boundaries\n",
    "ymin, ymax = ax.set_ylim(0, 1)\n",
    "for x in intercepts['_datetime_start']:\n",
    "    ax.plot([abcutils.core.pd2epoch(x), abcutils.core.pd2epoch(x)], [ymin, ymax], color='black', linestyle='--', linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(loci)\n",
    "loci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate UMAMI Diagrams Around Loci\n",
    "\n",
    "Generate UMAMI diagrams that _end_ at each locus and have `long_window` days' of benchmark data preceding them.  Don't bother creating UMAMI diagrams for benchmarks with fewer than `short_window` benchmark data in the preceding `long_window` days.\n",
    "\n",
    "Note that this process mixes up the semantic meaning of `long_window`.  When defining loci, `long_window` refers to a number of benchmark measurements, not days.  Ideally, one benchmark runs each day so this semantic difference is trivial.  However in reality, there are days when no benchmarks are run meaning loci are defined using a series of `long_window` benchmark measurements that often span _more than_ `long_window` days.\n",
    "\n",
    "Practically speaking, this does not change very much as long as the ratio of `long_window` in days to `long_window` in benchmark measurements is close to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_renders = 1\n",
    "\n",
    "print \"Rendering a maximum of %d UMAMI diagrams\" % max_renders\n",
    "\n",
    "rendered = 0\n",
    "for locus in loci.itertuples():\n",
    "    if rendered == max_renders:\n",
    "        break\n",
    "    region_idx0 = example_df.index.get_loc(locus.region_start)\n",
    "    region_idxf = example_df.index.get_loc(locus.region_end)\n",
    "    if locus.region_start == locus.Index and region_idx0 > 0:\n",
    "        region_idx0 -= 1\n",
    "\n",
    "    print example_df.iloc[region_idx0]['_datetime_start'], \\\n",
    "          example_df.index[region_idx0], \\\n",
    "          example_df.index[region_idxf]\n",
    "    umami_region = example_df.iloc[region_idx0:region_idxf]\n",
    "    if len(umami_region) >= min_measures:\n",
    "        abcutils.plot.generate_umami(umami_region, umami_rows.keys(), highlight_index=umami_region.index.get_loc(locus.Index))\n",
    "        rendered += 1\n",
    "    else:\n",
    "        print \"Skipping locus at %s because it has only %d data points (%d required)\" % (umami_region['_datetime_start'], len(umami_region), short_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loci = abcutils.features.generate_loci_sma(example_df,\n",
    "                                           plot_metric,\n",
    "                                           mins=True,\n",
    "                                           maxes=False,\n",
    "                                           short_window=short_window,\n",
    "                                           long_window=long_window)\n",
    "\n",
    "sma_intercepts = abcutils.features.sma_intercepts(example_df,\n",
    "                                                  plot_metric,\n",
    "                                                  short_window,\n",
    "                                                  long_window)\n",
    "\n",
    "ax = abcutils.plot.locus_summary(example_df,\n",
    "                                 plot_metric,\n",
    "                                 loci,\n",
    "                                 sma_intercepts=sma_intercepts,\n",
    "                                 align='edge',\n",
    "                                 regioncolors=['#0000000A', '#FFFFFF00'])\n",
    "\n",
    "# This corresponds to one of the UMAMIs identified above\n",
    "#ax.set_xlim(\n",
    "#    abcutils.core.pd2epoch(pandas.Timestamp(datetime.datetime(2017,  9, 21))),\n",
    "#    abcutils.core.pd2epoch(pandas.Timestamp(datetime.datetime(2017, 10,  1)))\n",
    "#)\n",
    "\n",
    "# This corresponds to a pretty region to graph\n",
    "ax.set_xlim(\n",
    "    abcutils.core.pd2epoch(pandas.Timestamp(datetime.datetime(2017,  8, 10))),\n",
    "    abcutils.core.pd2epoch(pandas.Timestamp(datetime.datetime(2017, 10, 3)))\n",
    ")\n",
    "\n",
    "# Add boundaries\n",
    "ymin, ymax = ax.get_ylim()\n",
    "for x in sma_intercepts['_datetime_start']:\n",
    "    ax.plot([abcutils.core.pd2epoch(x), abcutils.core.pd2epoch(x)], [ymin, ymax], color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "ax.grid(False)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.get_figure().set_size_inches(8,3)\n",
    "ax.set_ylabel(ax.get_ylabel().replace(\"\\n\", \" \"))\n",
    "abcutils.plot.fix_xticks_timeseries(ax, format=\"%b %d\", ha=\"center\", rotation=0, criteria=(lambda x: x.toordinal() % 10 == 0))\n",
    "\n",
    "ax.get_lines()[0].set_label(\"$SMA_{%d}$\" % short_window.days)\n",
    "ax.get_lines()[1].set_label(\"$SMA_{%d}$\" % long_window.days)\n",
    "ax.legend(loc='lower right',\n",
    "          bbox_to_anchor=(1.01, -0.04),\n",
    "          ncol=2,\n",
    "          handletextpad=0.1,\n",
    "          columnspacing=0.5)\n",
    "\n",
    "print \"%s on %s\" % (abcutils.CONFIG['benchmark_labels'].get(BENCHMARK_ID, BENCHMARK_ID), TEST_PLATFORM)\n",
    "\n",
    "output_file = \"figs/shortterm-%s-%s.pdf\" % (TEST_PLATFORM.split('@', 1)[0].replace('-', ''), BENCHMARK_ID.split('_', 1)[0])\n",
    "print \"Saving to\", output_file\n",
    "ax.get_figure().savefig(output_file, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
