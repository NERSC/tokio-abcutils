{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "import pandas\n",
    "import numpy\n",
    "import scipy.stats\n",
    "import abcutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Synthesize Data from CSV\n",
    "\n",
    "This process loads each summary CSV file, creates a few derived metrics, and then merges each system's CSV into a single global dataset that can be sliced and diced by system, benchmark, or any other way.  We are now caching the processed CSV in HDF5 format to speed up initial data ingest at the beginning of each analysis.  Delete the `CACHE_FILE` to re-generate this cache (e.g., when the contents of the CSV are updated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CACHE_FILE = 'cache.hdf5'\n",
    "if CACHE_FILE and os.path.isfile(CACHE_FILE):\n",
    "    print \"Loading from cache %s\" % CACHE_FILE\n",
    "    df = pandas.read_hdf(CACHE_FILE, 'summary')\n",
    "else:\n",
    "    df = pandas.concat([abcutils.load_and_synthesize_csv('summaries/edison-summaries_2017-02-14-2018-02-28.csv', system='edison'),\n",
    "                        abcutils.load_and_synthesize_csv('summaries/cori-summaries_2017-02-14-2018-02-28.csv', system='cori'),\n",
    "                        abcutils.load_and_synthesize_csv('summaries/alcf-tokio-results-2_14_17-2_15_18.csv', system='mira')],\n",
    "                       axis='rows')\n",
    "    if CACHE_FILE:\n",
    "        df.to_hdf(CACHE_FILE, key='summary', mode='w', format='fixed', complevel=9, complib='zlib')\n",
    "        print \"Cached synthesized CSV to %s\" % CACHE_FILE\n",
    "    \n",
    "# Reset the index to ensure that there are no degenerate indices in the final dataframe\n",
    "df.index = pandas.Index(data=numpy.arange(len(df)), dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd2epoch = lambda x: time.mktime(x.to_pydatetime().timetuple())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by = ['_test_platform', '_benchmark_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate a Single Test Platform\n",
    "\n",
    "Look at one combination of (compute system, file system, benchmark) to show what this UMAMI analysis can do.\n",
    "\n",
    "### Define Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_platform = 'scratch2@edison'\n",
    "test_platform = 'cscratch@cori-knl'\n",
    "# test_platform = 'cscratch@cori-haswell'\n",
    "# test_platform = 'mira-fs1@mira'\n",
    "\n",
    "# benchmark_id = 'ior_fpp_write'\n",
    "# benchmark_id = 'dbscan_read_shared_read'\n",
    "benchmark_id = 'vpicio_uni_shared_write'\n",
    "# benchmark_id = 'ior_shared_write'\n",
    "# benchmark_id = 'hacc_io_read_fpp_read'\n",
    "\n",
    "plot_metric = 'darshan_agg_perf_by_slowest_posix_gibs'\n",
    "date_start = datetime.datetime(2017, 2, 14)\n",
    "date_end = datetime.datetime(2018, 3, 1)\n",
    "group_by = ['_test_platform', '_benchmark_id']\n",
    "delta = datetime.timedelta(days=1).total_seconds()\n",
    "\n",
    "filtered_df = df.groupby(by=group_by).get_group((test_platform, benchmark_id))\n",
    "filtered_df = filtered_df[filtered_df['darshan_total_gibs_posix'] > 1.0]\n",
    "filtered_df = filtered_df[filtered_df['_datetime_start'] < date_end]\n",
    "filtered_df = filtered_df[filtered_df['_datetime_start'] >= date_start]\n",
    "\n",
    "print \"test_platform =\", test_platform\n",
    "print \"benchmark_id =\", abcutils.CONFIG['benchmark_labels'].get(benchmark_id, benchmark_id)\n",
    "print \"plot_metric =\", abcutils.CONFIG['metric_labels'].get(plot_metric, plot_metric)\n",
    "print \"date_start =\", date_start.isoformat()\n",
    "print \"date_end =\", date_end.isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Width of simple moving average (SMA) short/long windows\n",
    "short_window = 7\n",
    "long_window = 28\n",
    "\n",
    "print \"Short window will average over %d measurements at a time\" % short_window\n",
    "print \"Long window will average over %d measurements at a time\" % long_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics to include in UMAMI renderings and analysis.  Anything that\n",
    "# _might_ affect performance should be included here.\n",
    "umami_rows = [\n",
    "    'darshan_agg_perf_by_slowest_posix_gibs',\n",
    "    'coverage_factor_bw',\n",
    "    'coverage_factor_nodehrs',\n",
    "    'fs_ave_mds_cpu',\n",
    "    'fs_tot_metadata_ops',\n",
    "    'fs_ave_oss_cpu',\n",
    "    'fs_tot_open_ops',\n",
    "    'fshealth_ost_most_full_pct',\n",
    "    'fshealth_ost_overloaded_oss_count',\n",
    "    'jobsdb_concurrent_nodes',\n",
    "    'topology_job_max_radius',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Simple Moving Averages (SMAs)\n",
    "\n",
    "Compare a short-window SMA and a long-window SMA and use the places where they cross over to divide the entire year into _regions_ of interesting benchmark behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sma_intercepts = abcutils.features.sma_intercepts(filtered_df, plot_metric, short_window, long_window)\n",
    "sma_minmax = abcutils.features.sma_local_minmax(filtered_df,\n",
    "                                                plot_metric,\n",
    "                                                short_window,\n",
    "                                                long_window,\n",
    "                                                min_domain=7,\n",
    "                                                max_func=pandas.Series.idxmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each region defined above, find the _minimum_ performance observed and denote that measurement (and its associated job) as a _locus_.  We then collate all _loci_ into a set of poorly performing benchmarks that are worth contextualizing with UMAMI.\n",
    "\n",
    "We also plot the raw performance data (light blue bars), the short SMA (orange line), the long SMA (green line), and all loci (red bars) to visually verify that the loci we've identified are indeed poorly performing jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_raw = filtered_df['_datetime_start'].apply(lambda x: time.mktime(x.timetuple()))\n",
    "y_raw = filtered_df[plot_metric]\n",
    "\n",
    "loci_df = filtered_df.loc[sma_minmax.index]\n",
    "x_low = sma_minmax['_datetime_start'].apply(lambda x: time.mktime(x.timetuple()))\n",
    "y_low = sma_minmax[plot_metric]\n",
    "print \"Found %d loci across %s\" % (len(loci_df), date_end - date_start)\n",
    "\n",
    "### plot the raw data\n",
    "fig, ax = matplotlib.pyplot.subplots()\n",
    "fig.set_size_inches(16, 4)\n",
    "ax.grid()\n",
    "ax.bar(x_raw, y_raw, width=delta, alpha=0.5)\n",
    "ax.bar(x_low, y_low, width=delta, color='red')\n",
    "ax.set_ylabel(abcutils.CONFIG['metric_labels'].get(plot_metric, plot_metric).replace(\" \", \"\\n\"))\n",
    "ax.set_xticklabels([datetime.datetime.fromtimestamp(x).strftime(\"%b %d\") for x in ax.get_xticks()])\n",
    "fig.suptitle(\"%s on %s\" % (abcutils.CONFIG['benchmark_labels'].get(benchmark_id, benchmark_id),\n",
    "                           test_platform))\n",
    "\n",
    "### also calculate and plot the SMAs\n",
    "sma_short = abcutils.features.calculate_sma(filtered_df, '_datetime_start', plot_metric, short_window)\n",
    "sma_long = abcutils.features.calculate_sma(filtered_df, '_datetime_start', plot_metric, long_window)\n",
    "\n",
    "### plot the intercept points demarcating different regions\n",
    "#intercepts = numpy.array([(filtered_df['_datetime_start'].loc[x], filtered_df[plot_metric].loc[x]) for x in sma_intercepts.index])\n",
    "#x_intercept = [(x - numpy.datetime64('1970-01-01T00:00:00Z')) / numpy.timedelta64(1, 's') for x in intercepts[:, 0]]\n",
    "#y_intercept = intercepts[:, 1]\n",
    "#ax.scatter(x_intercept, y_intercept, color='red', marker='.')\n",
    "\n",
    "x_sma_short = [pd2epoch(x) for x in sma_short.index]\n",
    "y_sma_short = sma_short.values\n",
    "\n",
    "x_sma_long = [pd2epoch(x) for x in sma_long.index]\n",
    "y_sma_long = sma_long.values\n",
    "\n",
    "ax.plot(x_sma_short, y_sma_short, color='C1', linewidth=2)\n",
    "ax.plot(x_sma_long, y_sma_long, color='C2', linewidth=2)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate UMAMI Diagrams Around Loci\n",
    "\n",
    "Generate UMAMI diagrams that _end_ at each locus and have `long_window` days' of benchmark data preceding them.  Don't bother creating UMAMI diagrams for benchmarks with fewer than `short_window` benchmark data in the preceding `long_window` days.\n",
    "\n",
    "Note that this process mixes up the semantic meaning of `long_window`.  When defining loci, `long_window` refers to a number of benchmark measurements, not days.  Ideally, one benchmark runs each day so this semantic difference is trivial.  However in reality, there are days when no benchmarks are run meaning loci are defined using a series of `long_window` benchmark measurements that often span _more than_ `long_window` days.\n",
    "\n",
    "Practically speaking, this does not change very much as long as the ratio of `long_window` in days to `long_window` in benchmark measurements is close to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_renders = 1\n",
    "\n",
    "print \"Rendering a maximum of %d UMAMI diagrams\" % max_renders\n",
    "\n",
    "rendered = 0\n",
    "for row in sma_minmax.itertuples():\n",
    "    date_filter = filtered_df['_datetime_start'] <= row[1]\n",
    "    date_filter &= filtered_df['_datetime_start'] >= row[1] - datetime.timedelta(days=long_window)\n",
    "    umami_region = filtered_df[date_filter]\n",
    "    if len(umami_region) >= short_window:\n",
    "        abcutils.plot.generate_umami(umami_region, umami_rows)\n",
    "        rendered += 1\n",
    "        if rendered == max_renders:\n",
    "            break\n",
    "    else:\n",
    "        print \"Skipping locus at %s because it has only %d data points (%d required)\" % (umami_region['_datetime_start'], len(umami_region), short_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabulate frequency of different problems\n",
    "\n",
    "Because we define loci to be local minima (i.e., the worst benchmark measured in a temporally local region), all of the UMAMIs we generated above _should_ end on an extremely bad day.  To automatically identify the possible causes for bad performance at each locus, we look at all of the UMAMI metrics and flag those that also ended on extremely poor (e.g., worst quartile) values.  This is exactly the same process we used in the PDSW'17 paper's case studies, but now we have automated the process.\n",
    "\n",
    "With this method of flagging, we keep a running total of metrics that were flagged as possible culprits as we examine each locus.  Note that multiple metrics can be flagged for a single locus (e.g., low coverage factor _and_ high MDS load can both be flagged for a single benchmark run), so the sum of flags over all metrics will usually add up to more than the total number of loci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_root_cause(dataframe, dependent_column):\n",
    "    causes = []\n",
    "    for column in dataframe.columns:\n",
    "        big_is_good = abcutils.CONFIG['metric_big_is_good'].get(column, True)\n",
    "\n",
    "        result = None\n",
    "        if big_is_good:\n",
    "            cutoff = numpy.nanpercentile(dataframe[column].iloc[0:-1], 25)\n",
    "            if dataframe[column].iloc[-1] < cutoff:\n",
    "                result = (column, dataframe[column].iloc[-1], \"<\", cutoff)\n",
    "        else:\n",
    "            cutoff = numpy.nanpercentile(dataframe[column].iloc[0:-1], 75)\n",
    "            if dataframe[column].iloc[-1] > cutoff:\n",
    "                result = (column, dataframe[column].iloc[-1], \">\", cutoff)\n",
    "        \n",
    "        if column == dependent_column:\n",
    "            if result is None:\n",
    "                warnings.warn(\"%s (index %s) not in the worst quartile of %d values\" % (column, dataframe[column].index[-1], len(dataframe)))\n",
    "#               print dataframe[dependent_column]\n",
    "                return []\n",
    "        elif result:\n",
    "            causes.append(result)\n",
    "                \n",
    "    return causes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for row in sma_minmax.itertuples():\n",
    "    # look for rows indicating an extreme value that are preceded by a region boundary\n",
    "    date_filter = filtered_df['_datetime_start'] <= row[1]\n",
    "    date_filter &= filtered_df['_datetime_start'] >= row[1] - datetime.timedelta(days=long_window)\n",
    "    umami_region = filtered_df[date_filter][['_datetime_start'] + umami_rows]\n",
    "    if len(umami_region) >= short_window:\n",
    "        results['num_loci'] = results.get('num_loci', 0) + 1\n",
    "        for cause in classify_root_cause(umami_region[umami_rows], plot_metric):\n",
    "            results[cause[0]] = results.get(cause[0], 0) + 1\n",
    "num_loci = results.pop('num_loci')\n",
    "\n",
    "print \"Number of times each metric was flagged across %d loci:\" % num_loci\n",
    "for key in reversed(sorted(results.keys(), key=lambda x: results[x])):\n",
    "    print \"%3d %s\" % (results[key], abcutils.CONFIG['metric_labels'].get(key, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate root causes over all tests\n",
    "\n",
    "We now apply the above analysis to the entirety of the benchmark data across all systems.\n",
    "\n",
    "Note that warnings about certain loci not being in the worst quartile indicate that the SMA-based method we use to identify local minima is not perfect.  There are a variety of other methods (including some canned algorithms) that we can swap in to improve our classification of loci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Global plot parameters\n",
    "plot_metric = 'darshan_agg_perf_by_slowest_posix_gibs'\n",
    "date_start = datetime.datetime(2017, 2, 14)\n",
    "date_end = datetime.datetime(2018, 3, 1)\n",
    "group_by = ['_test_platform', '_benchmark_id']\n",
    "\n",
    "# Determine which plots to generate\n",
    "test_platforms = sorted(df['_test_platform'].unique())\n",
    "benchmark_ids = sorted(df['_benchmark_id'].unique())\n",
    "\n",
    "# test_platforms = ['cscratch@cori-knl']\n",
    "# benchmark_ids = ['dbscan_read_shared_read', 'vpicio_uni_shared_write']\n",
    "\n",
    "print \"plot_metric =\", abcutils.CONFIG['metric_labels'].get(plot_metric, plot_metric)\n",
    "print \"date_start =\", date_start.isoformat()\n",
    "print \"date_end =\", date_end.isoformat()\n",
    "\n",
    "grouped_df = df.groupby(by=group_by)\n",
    "\n",
    "results = {}\n",
    "for test_platform in test_platforms:\n",
    "    for benchmark_id in benchmark_ids:\n",
    "        try:\n",
    "            filtered_df = grouped_df.get_group((test_platform, benchmark_id))\n",
    "        except KeyError:\n",
    "            continue\n",
    "        filtered_df = filtered_df[filtered_df['darshan_total_gibs_posix'] > 1.0]\n",
    "        filtered_df = filtered_df[filtered_df['_datetime_start'] < date_end]\n",
    "        filtered_df = filtered_df[filtered_df['_datetime_start'] >= date_start]\n",
    "\n",
    "        sma_minmax = abcutils.features.sma_local_minmax(filtered_df,\n",
    "                                                        plot_metric,\n",
    "                                                        short_window,\n",
    "                                                        long_window,\n",
    "                                                        min_domain=short_window,\n",
    "                                                        max_func=pandas.Series.idxmin)\n",
    "#       print \"Found %d loci for %s, %s\" % (len(sma_minmax), test_platform, benchmark_id)\n",
    "        for row in sma_minmax.itertuples():\n",
    "            date_filter = filtered_df['_datetime_start'] <= row[1]\n",
    "            date_filter &= filtered_df['_datetime_start'] >= row[1] - datetime.timedelta(days=long_window)\n",
    "            umami_region = filtered_df[date_filter][['_datetime_start'] + umami_rows]\n",
    "            if len(umami_region) >= short_window:\n",
    "                results['num_loci'] = results.get('num_loci', 0) + 1\n",
    "                for cause in classify_root_cause(umami_region[umami_rows], plot_metric):\n",
    "                    results[cause[0]] = results.get(cause[0], 0) + 1\n",
    "                    \n",
    "num_loci = results.pop('num_loci')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following bar graph shows the total number of times each metric has been flagged as a possible source of performance loss as defined above: its value was \"bad\" coincident with each locus, where a locus is a job whose performance was abnormally poor and \"bad\" is defined as being within the 25th worst percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = matplotlib.pyplot.subplots()\n",
    "fig.set_size_inches(16,4)\n",
    "\n",
    "x_labels = list(reversed(sorted(results.keys(), key=lambda x: results[x])))\n",
    "y_values = [results[x] for x in x_labels]\n",
    "x_values = numpy.arange(len(y_values))\n",
    "\n",
    "ax.yaxis.grid(True)\n",
    "ax.bar(x_values, y_values)\n",
    "ax.set_xticks(x_values)\n",
    "ax.set_xticklabels([abcutils.CONFIG['metric_labels'].get(x, x) for x in x_labels], rotation=45, ha='right')\n",
    "ax.set_ylabel(\"Number of occurrences\")\n",
    "ax.set_title(\"Hypothesized Causes of Performance Degradation (%d Jobs Total)\" % num_loci)\n",
    "\n",
    "for index, x_value in enumerate(x_values):\n",
    "    ax.annotate(\"%d\" % y_values[index], xy=(x_value, y_values[index]), ha='center')\n",
    "            \n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many caveats with the above plot; notably, the majority of jobs were run on Lustre since this data includes all Edison, Cori+KNL, and Cori+Haswell jobs.  In addition, the Mira data does not currently contain file system health data (although it is available), so the \"Number of Overloaded OSSes\" may be underreported.\n",
    "\n",
    "The most appropriate way to present this data is to produce one bar graph per test platform (compute system + file system combination) so that metrics that are only available on one test platform aren't being directly compared with others that are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
