{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "import time\n",
    "import datetime\n",
    "import pandas\n",
    "import numpy\n",
    "import scipy.stats\n",
    "import abcutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PLATFORMS = [\n",
    "    'scratch1@edison',\n",
    "    'scratch2@edison',\n",
    "    'scratch3@edison',\n",
    "    'cscratch@cori-knl',\n",
    "    'cscratch@cori-haswell',\n",
    "    'mira-fs1@mira'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Synthesize Data from CSV\n",
    "\n",
    "This process loads each summary CSV file, creates a few derived metrics, and then merges each system's CSV into a single global dataset that can be sliced and diced by system, benchmark, or any other way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pandas.concat([abcutils.load_and_synthesize_csv('summaries/edison-summaries_2017-02-14-2017-12-30.csv', system='edison'),\n",
    "                    abcutils.load_and_synthesize_csv('summaries/cori-summaries_2017-02-14-2017-12-31.csv', system='cori'),\n",
    "                    abcutils.load_and_synthesize_csv('summaries/alcf-tokio-results-2_14_17-2_15_18.csv', system='mira')],\n",
    "                   axis='rows')\n",
    "\n",
    "# Reset the index to ensure that there are no degenerate indices in the final dataframe\n",
    "df.index = pandas.Index(data=numpy.arange(len(df)), dtype='int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting important metrics\n",
    "\n",
    "While we can go at this correlation analysis in an unsupervised way, in practice, there are just too many metrics to walk through manually, and the level of redundancy across many metrics makes them unhelpful for statistical analysis anyway.  Although we are definitely biasing our results by choosing only a few metrics of interest, we can justify this by bootstrapping off of the findings of the PDSW'17 paper and say that we previously showed which metrics are most interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in sorted(df.columns):\n",
    "#     print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERESTING_METRICS = [\n",
    "    '_benchmark_id',\n",
    "    '_datetime_end',\n",
    "    '_datetime_start',\n",
    "    '_file_system',\n",
    "    '_subsystem',\n",
    "    '_system',\n",
    "    '_test_platform',\n",
    "    'coverage_factor_bw',\n",
    "    'coverage_factor_read_bw',\n",
    "    'coverage_factor_write_bw',\n",
    "    'darshan_agg_perf_by_slowest_posix_gibs',\n",
    "    'darshan_fpp_job?',\n",
    "    'darshan_jobid',\n",
    "    'darshan_normalized_perf_by_max',\n",
    "    'darshan_write_job?',\n",
    "    'fs_ave_mds_cpu',\n",
    "    'fs_ave_oss_cpu',\n",
    "    'fs_frac_missing',\n",
    "    'fs_max_mds_cpu',\n",
    "    'fs_max_oss_cpu',\n",
    "    'fs_max_gibs_read_per_sec',\n",
    "    'fs_max_gibs_written_per_sec',\n",
    "    'fs_tot_openclose_ops',\n",
    "    'fs_tot_metadata_ops',\n",
    "    'fs_tot_readdir_ops',\n",
    "    'fs_tot_unlink_ops',\n",
    "    'fs_tot_getattr_ops',\n",
    "    'fshealth_ost_avg_full_pct',\n",
    "    'fshealth_ost_most_full_pct',\n",
    "    'fshealth_ost_overloaded_pct',\n",
    "    'jobsdb_concurrent_jobs',\n",
    "    'topology_job_avg_radius',\n",
    "    'topology_job_max_radius',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Feature Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_platform = 'mira-fs1@mira'\n",
    "benchmark_id = 'ior_fpp_write'\n",
    "plot_metric = 'darshan_normalized_perf_by_max'\n",
    "date_start = datetime.datetime(2017, 2, 14)\n",
    "date_end = date_start + datetime.timedelta(days=365)\n",
    "min_streak = 3 # in days by default\n",
    "group_by = ['_test_platform', '_benchmark_id']\n",
    "\n",
    "filtered_df = df.groupby(by=group_by).get_group((test_platform, benchmark_id))\n",
    "filtered_df = filtered_df[filtered_df['_datetime_start'] < date_end]\n",
    "filtered_df = filtered_df[filtered_df['_datetime_start'] >= date_start]\n",
    "\n",
    "print \"test_platform =\", test_platform\n",
    "print \"benchmark_id =\", abcutils.CONFIG['benchmark_labels'].get(benchmark_id, benchmark_id)\n",
    "print \"plot_metric =\", abcutils.CONFIG['metric_labels'].get('darshan_normalized_perf_by_max', plot_metric)\n",
    "print \"date_start =\", date_start.isoformat()\n",
    "print \"date_end =\", date_end.isoformat()\n",
    "print \"Minimum days to count as a streak =\", min_streak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we plot the raw data for the (`test_platform`, `benchmark_id`) of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = matplotlib.pyplot.subplots(nrows=2, ncols=1, sharex=True)\n",
    "fig.set_size_inches(20,8)\n",
    "\n",
    "x_raw = filtered_df['_datetime_start'].apply(lambda x: time.mktime(x.timetuple()))\n",
    "y_raw = filtered_df['darshan_normalized_perf_by_max']\n",
    "\n",
    "ax = axes[0]\n",
    "ax.grid()\n",
    "ax.set_ylim(0, 1)\n",
    "ax.plot(x_raw,\n",
    "        y_raw,\n",
    "        linestyle='-',\n",
    "        marker='.')\n",
    "ax.set_ylabel(abcutils.CONFIG['metric_labels'].get('darshan_normalized_perf_by_max', 'darshan_normalized_perf_by_max'))\n",
    "ax.set_xticklabels([datetime.datetime.fromtimestamp(x).strftime(\"%Y-%m-%d\") for x in ax.get_xticks()])\n",
    "ax.set_title(\"%s on %s\" % (abcutils.CONFIG['benchmark_labels'].get(benchmark_id, benchmark_id),\n",
    "                           test_platform))\n",
    "\n",
    "abcutils.plot.timeseries_boxplot(filtered_df, plot_metric, date_start, date_end, ax=axes[1])\n",
    "axes[1].set_ylabel(abcutils.CONFIG['metric_labels'].get('darshan_normalized_perf_by_max', 'darshan_normalized_perf_by_max'))\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Streaks\n",
    "\n",
    "Now we find cases where performance monotonically increased or decreased over the course of several consecutive days.  In the following graph, streaks of **decreasing** performance are red and streaks of **increasing** performance are green.  Gaps in these red/green lines are regions where data was not streaking up or down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = matplotlib.pyplot.subplots()\n",
    "fig.set_size_inches(20,4)\n",
    "\n",
    "ax = abcutils.plot.timeseries_boxplot(filtered_df, plot_metric, date_start, date_end, ax=ax)\n",
    "\n",
    "streaks = abcutils.features.find_streaks_df(filtered_df, plot_metric, min_streak=min_streak)\n",
    "abcutils.plot.timeseries_streaks(filtered_df, streaks, ax)\n",
    "\n",
    "xlabel = \"Week\"\n",
    "ylabel = \"%s\\n(%s)\" % (abcutils.CONFIG['benchmark_labels'].get(benchmark_id, benchmark_id),\n",
    "                       abcutils.CONFIG['metric_labels'].get(plot_metric, plot_metric))\n",
    "ax.set_xlabel(xlabel)\n",
    "ax.set_ylabel(ylabel)\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_title(\"%s on %s\" % (abcutils.CONFIG['benchmark_labels'].get(benchmark_id, benchmark_id),\n",
    "                           test_platform))\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding window slopes\n",
    "\n",
    "We can also calculate the slope of all data points within a fixed-width window that slides by some smaller delta.  Filtering data points that fall within windows that demonstrate a dramatic slope may be another way to identify periods of time that are interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "window_width = datetime.timedelta(days=7)\n",
    "window_slide = datetime.timedelta(days=1)\n",
    "print \"Window width is %s\" % window_width\n",
    "print \"Window slides by %s\" % window_slide\n",
    "\n",
    "fig, ax = matplotlib.pyplot.subplots()\n",
    "fig.set_size_inches(20,4)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(True)\n",
    "\n",
    "streaks = abcutils.features.sliding_window_slopes(filtered_df, plot_metric, date_start, date_end, width=datetime.timedelta(days=7), delta=datetime.timedelta(days=1))\n",
    "abcutils.plot.timeseries_manylines(streaks, colorfunc=lambda x: 'green' if x[1][-1] > x[1][0] else 'red', ax=ax, linewidth=2)\n",
    "\n",
    "date = date_start\n",
    "xticks = []\n",
    "xticklabels = []\n",
    "while date + window_width < date_end:\n",
    "    xticks.append(time.mktime(date.timetuple()))\n",
    "    xticklabels.append(time.strftime(\"%Y-%m-%d\"))\n",
    "    date += window_width\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_xticklabels(xticklabels, rotation=90)\n",
    "ax.set_title(\"%s on %s\" % (abcutils.CONFIG['benchmark_labels'].get(benchmark_id, benchmark_id),\n",
    "                           test_platform))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Feature Detection Methods\n",
    "\n",
    "Now plot all of these feature detect approaches in a single pane to visually identify periods of time when data is consistently anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_detections(dataframe, test_platform, benchmark_id, plot_metric, min_streak=5):\n",
    "    try:\n",
    "        filtered_dataframe = dataframe.groupby(by=group_by).get_group((test_platform, benchmark_id))\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "    NROWS = 4\n",
    "    fig, axes = matplotlib.pyplot.subplots(nrows=NROWS, ncols=1, sharex=True)\n",
    "    fig.set_size_inches(20,4 * NROWS)\n",
    "    xlabel = \"Week\"\n",
    "    ylabel = \"%s\\n(%s)\" % (abcutils.CONFIG['benchmark_labels'].get(benchmark_id, benchmark_id),\n",
    "                           abcutils.CONFIG['metric_labels'].get(plot_metric, plot_metric))\n",
    "\n",
    "    # Row #1\n",
    "    ax = axes[0]\n",
    "    ax.plot(filtered_dataframe['_datetime_start'].apply(lambda x: time.mktime(x.timetuple())),\n",
    "            filtered_dataframe['darshan_normalized_perf_by_max'],\n",
    "            linestyle='-',\n",
    "            marker='.')\n",
    "    ax.set_title(\"Raw Data\")\n",
    "\n",
    "    # Row #2\n",
    "    ax = axes[1]\n",
    "    abcutils.plot.timeseries_boxplot(filtered_dataframe, plot_metric, date_start, date_end, ax=ax)\n",
    "    ax.set_title(\"Distribution over time\")\n",
    "\n",
    "    # Row #3\n",
    "    ax = axes[2]\n",
    "    streaks = abcutils.features.find_streaks_df(filtered_dataframe, plot_metric, min_streak=min_streak)\n",
    "    abcutils.plot.timeseries_streaks(filtered_dataframe, streaks, ax=ax)\n",
    "    ax.set_title(\"Monotonic streaks (%d or more consecutive days)\" % min_streak)\n",
    "\n",
    "    # Row #4\n",
    "    ax = axes[3]\n",
    "    width = datetime.timedelta(days=7)\n",
    "    delta = datetime.timedelta(days=1)\n",
    "    streaks = abcutils.features.sliding_window_slopes(filtered_dataframe,\n",
    "                                                      plot_metric,\n",
    "                                                      date_start,\n",
    "                                                      date_end,\n",
    "                                                      width=width,\n",
    "                                                      delta=delta)\n",
    "    abcutils.plot.timeseries_manylines(streaks,\n",
    "                                       colorfunc=lambda x: 'green' if x[1][-1] > x[1][0] else 'red',\n",
    "                                       ax=ax,\n",
    "                                       linewidth=2)\n",
    "    ax.set_title(\"Sliding-window slope (%d-day windows sliding %d days at a time)\" % (width.days, delta.days))\n",
    "\n",
    "    # Set ticks and labels\n",
    "    date = date_start\n",
    "    xticks = []\n",
    "    xticklabels = []\n",
    "    while date + window_width < date_end:\n",
    "        xticks.append(time.mktime(date.timetuple()))\n",
    "        xticklabels.append(date.strftime(\"%Y-%m-%d\"))\n",
    "        date += window_width\n",
    "    ax.set_xlabel(xlabel)\n",
    "\n",
    "    # Common settings on all axes\n",
    "    for ax in axes:\n",
    "        ax.set_ylim(0, 1.2)\n",
    "        ax.xaxis.grid(True)\n",
    "        ax.yaxis.grid(True)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_xticks(xticks)\n",
    "        ax.set_xticklabels(xticklabels, rotation=90)\n",
    "        ax.set_title(ax.get_title(), **{'x': 0.01, 'y': 0.02, 'horizontalalignment': 'left'})\n",
    "\n",
    "    fig.suptitle(\"%s on %s\" % (abcutils.CONFIG['benchmark_labels'].get(benchmark_id, benchmark_id),\n",
    "                               test_platform))\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    \n",
    "    return axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Global plot parameters\n",
    "plot_metric = 'darshan_normalized_perf_by_max'\n",
    "date_start = datetime.datetime(2017, 2, 14)\n",
    "date_end = date_start + datetime.timedelta(days=365)\n",
    "min_streak = 5 # in days by default\n",
    "group_by = ['_test_platform', '_benchmark_id']\n",
    "\n",
    "# Determine which plots to generate\n",
    "test_platforms = sorted(df['_test_platform'].unique())\n",
    "# benchmark_ids = sorted(df['_benchmark_id'].unique())\n",
    "benchmark_ids = ['ior_shared_write']\n",
    "\n",
    "# Generate plots\n",
    "for test_platform in test_platforms:\n",
    "    for benchmark_id in benchmark_ids:\n",
    "        axes = plot_feature_detections(df, test_platform, benchmark_id, plot_metric, min_streak)\n",
    "        axes[0].get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlating during streaks\n",
    "\n",
    "Now that we have a few ways to classify benchmark data points, we can look for interesting correlations among the different classifications.  We first apply a binary classification where each benchmark measurement is a member of a streak or it is not.  Then we correlate with performance in the case where performance **was** on a streak and when it **was not** on a streak (it was bouncing up and down instead).\n",
    "\n",
    "In the clustered bar graphs, black and yellow just visually separate different clusters.  Red, green, and blue outlines indicate the confidence of the correlation coefficient plotted; red is low confidence, green is medium, and blue is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby(by=['_test_platform', '_benchmark_id'])\n",
    "streak_indices = [False] * len(df.index)\n",
    "\n",
    "for (test_platform, benchmark_id), df_group in grouped_df:\n",
    "    streaks = abcutils.features.find_streaks_df(df_group, 'darshan_normalized_perf_by_max', min_streak=5)\n",
    "    for streak in streaks:\n",
    "        for index in streak[0]:\n",
    "            streak_indices[index] = True\n",
    "\n",
    "# Convert to array so we can do `streak_indices == True`\n",
    "streak_indices = numpy.array(streak_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by = ['_test_platform', '_benchmark_id']\n",
    "\n",
    "correlation_dfs = {\n",
    "    True: None,\n",
    "    False: None\n",
    "}\n",
    "correlation_dfs_labels = {\n",
    "    True: \"during streaks\",\n",
    "    False: \"not during streaks\"\n",
    "}\n",
    "for fs in TEST_PLATFORMS:\n",
    "    for streaking in sorted(correlation_dfs.keys()):\n",
    "        df_filter = (df['_test_platform'] == fs)\n",
    "        df_filter &= (streak_indices == streaking)\n",
    "        df_to_correlate = df[df_filter][INTERESTING_METRICS]\n",
    "        if len(df_to_correlate) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate the correlation data for this vector\n",
    "        correlation = abcutils.correlation.calc_correlation_vector(\n",
    "            df_to_correlate,\n",
    "            correlate_with='darshan_normalized_perf_by_max')\n",
    "\n",
    "        # rename the columns in this vector to include the file system name\n",
    "        new_cols = {}\n",
    "        for index, col_name in enumerate(correlation.columns):\n",
    "            new_cols[col_name] = \"%s %s\" % (fs, col_name)\n",
    "        correlation.rename(columns=new_cols, inplace=True)\n",
    "\n",
    "        # join the vector to the previous vectors' dataframe\n",
    "        if correlation_dfs[streaking] is None:\n",
    "            correlation_dfs[streaking] = correlation\n",
    "        else:\n",
    "            correlation_dfs[streaking] = pandas.concat([correlation_dfs[streaking], correlation], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for streaking in sorted(correlation_dfs.keys()):\n",
    "#    correlations = correlation_dfs[streaking]\n",
    "#    ax = abcutils.plot.correlation_vector_table(\n",
    "#        correlations,\n",
    "#        row_name_map=abcutils.CONFIG['metric_labels'])\n",
    "#\n",
    "#    # Set the table width larger if displaying lots of metrics\n",
    "#    ax.get_figure().set_size_inches(20, 0.4 * len(correlations))\n",
    "#    ax.set_title(\"Streaking\" if streaking else \"Not Streaking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print \"\"\"Each cluster of bars represents non-overlapping subsets of the benchmark data.\n",
    "The bars are printed in the following order:\n",
    "\"\"\"\n",
    "print \"\\n\".join([correlation_dfs_labels.get(x) for x in sorted(correlation_dfs_labels.keys())])\n",
    "\n",
    "plot_metrics = correlation_dfs.itervalues().next().index\n",
    "for test_platform in sorted(df['_test_platform'].unique()):\n",
    "    ax = abcutils.plot.clustered_correlation_bars(correlation_dfs, plot_metrics, test_platform)\n",
    "    ax.set_title(test_platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlating only the top and bottom quartiles\n",
    "\n",
    "Instead of correlating against areas where performance increased or decreased over a long period of time, we divide the benchmark measurements into three groups:\n",
    "\n",
    "1. the bottom 25% worst measurements\n",
    "2. the middle 50% of measurements (all measurements between the first and third quartile)\n",
    "3. the top 25% best measurements\n",
    "\n",
    "Then we do correlation analysis within each group to see if really bad performance shows correlation with metrics in a way that is not captured for the majority of jobs that had so-so performance.\n",
    "\n",
    "Unlike the streak-based correlation, we lose all temporal information by classifying data this way.  However if the correlations are more compelling, it may suggest that long-term (temporally correlated) performance maladies are not the primary cause of extreme performance variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by = ['_test_platform', '_benchmark_id']\n",
    "metric_distributions = df.groupby(by=group_by).describe()\n",
    "\n",
    "correlation_dfs = {\n",
    "    \"25%\": None,\n",
    "    \"50%\": None,\n",
    "    \"75%\": None\n",
    "}\n",
    "correlation_dfs_labels = {\n",
    "    \"25%\": \"bottom quartile of runs\",\n",
    "    \"50%\": \"middle 50% of runs\",\n",
    "    \"75%\": \"top quartile of runs\"\n",
    "}\n",
    "\n",
    "for quartile in sorted(correlation_dfs.keys()):\n",
    "    for fs in TEST_PLATFORMS:\n",
    "        df_to_correlate = df[df['_test_platform'] == fs][INTERESTING_METRICS]\n",
    "\n",
    "        if quartile in ['25%', '75%']:\n",
    "            cutoff = metric_distributions.loc[fs, benchmark_id]['darshan_normalized_perf_by_max'][quartile]\n",
    "            cutoff_filter = (df_to_correlate['darshan_normalized_perf_by_max'] < cutoff)\n",
    "        elif quartile == '50%': # actually the IQR\n",
    "            cutoff_low = metric_distributions.loc[fs, benchmark_id]['darshan_normalized_perf_by_max']['25%']\n",
    "            cutoff_high = metric_distributions.loc[fs, benchmark_id]['darshan_normalized_perf_by_max']['75%']\n",
    "            cutoff_filter = (df_to_correlate['darshan_normalized_perf_by_max'] > cutoff_low)\n",
    "            cutoff_filter &= (df_to_correlate['darshan_normalized_perf_by_max'] > cutoff_high)\n",
    "            \n",
    "        # Calculate the correlation data for this vector\n",
    "        correlation = abcutils.correlation.calc_correlation_vector(\n",
    "            df_to_correlate[cutoff_filter],\n",
    "            correlate_with='darshan_normalized_perf_by_max')\n",
    "\n",
    "        # rename the columns in this vector to include the file system name\n",
    "        new_cols = {}\n",
    "        for index, col_name in enumerate(correlation.columns):\n",
    "            new_cols[col_name] = \"%s %s\" % (fs, col_name)\n",
    "        correlation.rename(columns=new_cols, inplace=True)\n",
    "\n",
    "        # join the vector to the previous vectors' dataframe\n",
    "        if correlations is None:\n",
    "            correlation_dfs[quartile] = correlation\n",
    "        else:\n",
    "            correlation_dfs[quartile] = pandas.concat([correlation_dfs[quartile], correlation], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for quartile in sorted(correlation_dfs.keys()):\n",
    "#    correlations = correlation_dfs[quartile]\n",
    "#    ax = abcutils.plot.correlation_vector_table(\n",
    "#        correlations,\n",
    "#        row_name_map=abcutils.CONFIG['metric_labels'])\n",
    "#\n",
    "#    # Set the table width larger if displaying lots of metrics\n",
    "#    ax.get_figure().set_size_inches(20, 0.4 * len(correlations))\n",
    "#    ax.set_title(quartile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print \"\"\"Each cluster of bars represents non-overlapping subsets of the benchmark data.\n",
    "The bars are printed in the following order:\n",
    "\"\"\"\n",
    "print \"\\n\".join([correlation_dfs_labels.get(x) for x in sorted(correlation_dfs_labels.keys())])\n",
    "\n",
    "plot_metrics = correlation_dfs.itervalues().next().index\n",
    "for test_platform in sorted(df['_test_platform'].unique()):\n",
    "    ax = abcutils.plot.clustered_correlation_bars(correlation_dfs, plot_metrics, test_platform)\n",
    "    ax.set_title(test_platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
