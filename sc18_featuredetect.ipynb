{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import time\n",
    "import datetime\n",
    "import pandas\n",
    "import numpy\n",
    "import scipy.stats\n",
    "import abcutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Analysis Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PLATFORMS = [\n",
    "    'scratch1@edison',\n",
    "    'scratch2@edison',\n",
    "    'scratch3@edison',\n",
    "    'cscratch@cori-knl',\n",
    "    'cscratch@cori-haswell',\n",
    "    'mira-fs1@mira'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Synthesize Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pandas.concat([abcutils.load_and_synthesize_csv('summaries/edison-summaries_2017-02-14-2017-12-30.csv', system='edison'),\n",
    "                    abcutils.load_and_synthesize_csv('summaries/cori-summaries_2017-02-14-2017-12-31.csv', system='cori'),\n",
    "                    abcutils.load_and_synthesize_csv('summaries/alcf-tokio-results-2_14_17-2_15_18.csv', system='mira')],\n",
    "                   axis='rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Feature Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Streaks\n",
    "\n",
    "Find cases where performance monotonically increased or decreased over the course of several consecutive days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"\"\"\n",
    "Valid benchmark_id values:\n",
    "=========================\"\"\"\n",
    "print \"\\n\".join(df['_benchmark_id'].unique())\n",
    "\n",
    "print \"\"\"\n",
    "Valid test_platform values:\n",
    "==========================\"\"\"\n",
    "print \"\\n\".join(df['_test_platform'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_platform = 'cscratch@cori-knl'\n",
    "benchmark_id = 'ior_fpp_read'\n",
    "plot_metric = 'darshan_normalized_perf_by_max'\n",
    "date_start = datetime.datetime(2017, 2, 14)\n",
    "date_end = date_start + datetime.timedelta(days=365)\n",
    "min_streak = 3 # in days by default\n",
    "group_by = [ '_test_platform', '_benchmark_id' ]\n",
    "\n",
    "filtered_df = df.groupby(by=group_by).get_group((test_platform, benchmark_id))\n",
    "filtered_df = filtered_df[filtered_df['_datetime_start'] < date_end]\n",
    "filtered_df = filtered_df[filtered_df['_datetime_start'] >= date_start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = matplotlib.pyplot.subplots()\n",
    "fig.set_size_inches(20,4)\n",
    "\n",
    "ax = abcutils.plot.timeseries_boxplot(filtered_df, plot_metric, date_start, date_end, ax=ax)\n",
    "\n",
    "streaks = abcutils.features.find_streaks(filtered_df[plot_metric], min_streak=min_streak)\n",
    "for streak in streaks:\n",
    "    x = [time.mktime((filtered_df.iloc[x]['_datetime_start']).timetuple()) for x in streak[0]]\n",
    "    if streak[1][-1] > streak[1][0]:\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    ax.plot(x,\n",
    "            streak[1],\n",
    "            marker='',\n",
    "            linestyle='-',\n",
    "            linewidth=4,\n",
    "            color=color,\n",
    "            markersize=5,\n",
    "            markerfacecolor=color)\n",
    "xlabel = \"Week\"\n",
    "ylabel = \"%s\\n(%s)\" % (abcutils.CONFIG['benchmark_labels'].get(benchmark_id, benchmark_id),\n",
    "                       abcutils.CONFIG['metric_labels'].get(plot_metric, plot_metric))\n",
    "ax.set_xlabel(xlabel)\n",
    "ax.set_ylabel(ylabel)\n",
    "ax.set_ylim(0,1)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting important metrics\n",
    "\n",
    "While we can go at this correlation analysis in an unsupervised way, in practice, there are just too many metrics to walk through manually, and the level of redundancy in the data makes a lot of the metrics meaningless anyway.  Although we are definitely biasing our results by choosing only a few metrics of interest, we can justify this by bootstrapping off of the findings of the PDSW'17 paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sorted(df.columns):\n",
    "    print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERESTING_METRICS = [\n",
    "    '_benchmark_id',\n",
    "    '_datetime_end',\n",
    "    '_datetime_start',\n",
    "    '_file_system',\n",
    "    '_subsystem',\n",
    "    '_system',\n",
    "    '_test_platform',\n",
    "    'coverage_factor_bw',\n",
    "    'coverage_factor_read_bw',\n",
    "    'coverage_factor_write_bw',\n",
    "    'darshan_agg_perf_by_slowest_posix_gibs',\n",
    "    'darshan_fpp_job?',\n",
    "    'darshan_jobid',\n",
    "    'darshan_normalized_perf_by_max',\n",
    "    'darshan_write_job?',\n",
    "    'fs_ave_mds_cpu',\n",
    "    'fs_ave_oss_cpu',\n",
    "    'fs_frac_missing',\n",
    "    'fs_max_mds_cpu',\n",
    "    'fs_max_oss_cpu',\n",
    "    'fs_max_gibs_read_per_sec',\n",
    "    'fs_max_gibs_written_per_sec',\n",
    "    'fs_tot_openclose_ops',\n",
    "    'fs_tot_metadata_ops',\n",
    "    'fs_tot_readdir_ops',\n",
    "    'fs_tot_unlink_ops',\n",
    "    'fs_tot_getattr_ops',\n",
    "    'fshealth_ost_avg_full_pct',\n",
    "    'fshealth_ost_most_full_pct',\n",
    "    'fshealth_ost_overloaded_pct',\n",
    "    'jobsdb_concurrent_jobs',\n",
    "    'topology_job_avg_radius',\n",
    "    'topology_job_max_radius',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlating during streaks\n",
    "\n",
    "Look for interesting correlations among the filtered data.  First calculate the correlation vectors for each test platform and assemble a dataframe from those correlation vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "correlations = None\n",
    "for fs in TEST_PLATFORMS:\n",
    "    # Start with a single file system worth of data\n",
    "    df_to_correlate = df[(df['_test_platform'] == fs) & (df['_benchmark_id'] == benchmark_id)][INTERESTING_METRICS]\n",
    "\n",
    "    # Find streaks from the dataframe\n",
    "    streaks = abcutils.features.find_streaks(df_to_correlate[plot_metric], min_streak=min_streak)\n",
    "    \n",
    "    # Build a filter vector that only returns data from streaks\n",
    "    up_filter = [False] * len(df_to_correlate)\n",
    "    down_filter = [False] * len(df_to_correlate)\n",
    "    for streak in streaks:\n",
    "        if streak[1][-1] > streak[1][0]:\n",
    "            for index in streak[0]:\n",
    "                up_filter[index] = True\n",
    "        else:\n",
    "            for index in streak[0]:\n",
    "                down_filter[index] = True\n",
    "#   streak_filter = numpy.array(up_filter) | numpy.array(down_filter)\n",
    "    streak_filter = numpy.array(down_filter)\n",
    "\n",
    "    # Calculate the correlation data for this vector\n",
    "    correlation = abcutils.correlation.calc_correlation_vector(\n",
    "        df_to_correlate[streak_filter],\n",
    "        correlate_with='darshan_normalized_perf_by_max')\n",
    "    \n",
    "    # rename the columns in this vector to include the file system name\n",
    "    new_cols = {}\n",
    "    for index, col_name in enumerate(correlation.columns):\n",
    "        new_cols[col_name] = \"%s %s\" % (fs, col_name)\n",
    "    correlation.rename(columns=new_cols, inplace=True)\n",
    "    \n",
    "    # join the vector to the previous vectors' dataframe\n",
    "    if correlations is None:\n",
    "        correlations = correlation\n",
    "    else:\n",
    "        correlations = pandas.concat([correlations, correlation], axis='columns')\n",
    "\n",
    "# Only draw metrics that show something interesting\n",
    "confidence_filter = None\n",
    "for fs in TEST_PLATFORMS:\n",
    "    subfilter = correlations['%s p-value' % fs] < 1.0e-5\n",
    "    subfilter = (correlations['%s coefficient' % fs] > 0.30) | (correlations['%s coefficient' % fs] < -0.30)\n",
    "    if confidence_filter is None:\n",
    "        confidence_filter = subfilter\n",
    "    else:\n",
    "        confidence_filter |= subfilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the actual table\n",
    "ax = abcutils.plot.correlation_vector_table(\n",
    "    correlations[confidence_filter],\n",
    "    row_name_map=abcutils.CONFIG['metric_labels'])\n",
    "\n",
    "# Set the table width larger if displaying lots of metrics\n",
    "ax.get_figure().set_size_inches(18, 0.4 * len(correlations[confidence_filter]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlating only the top and bottom quartiles\n",
    "\n",
    "Instead of correlating against areas where performance increased or decreased over a long period of time, we just take the top and/or bottom quartiles and correlate across the worst and/or best observed performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_metrics_grouped = df.groupby(by=group_by).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summarized_metrics_grouped.loc['scratch1@edison', 'ior_fpp_read']['coverage_factor_bw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by = ['_test_platform', '_benchmark_id']\n",
    "summarized_metrics_grouped = df.groupby(by=group_by).describe()\n",
    "\n",
    "correlations = None\n",
    "for fs in TEST_PLATFORMS:\n",
    "    # Start with a single file system worth of data\n",
    "#   df_to_correlate = df[(df['_test_platform'] == fs) & (df['_benchmark_id'] == benchmark_id)]\n",
    "    df_to_correlate = df[df['_test_platform'] == fs][INTERESTING_METRICS]\n",
    "    \n",
    "    bottom_cutoff = summarized_metrics_grouped.loc[fs, benchmark_id]['darshan_normalized_perf_by_max']['25%']\n",
    "    median_cutoff = summarized_metrics_grouped.loc[fs, benchmark_id]['darshan_normalized_perf_by_max']['25%']\n",
    "    top_cutoff = summarized_metrics_grouped.loc[fs, benchmark_id]['darshan_normalized_perf_by_max']['75%']\n",
    "#   cutoff_filter = (df_to_correlate['darshan_normalized_perf_by_max'] < bottom_cutoff)\n",
    "#   cutoff_filter = (df_to_correlate['darshan_normalized_perf_by_max'] > top_cutoff)\n",
    "    cutoff_filter = (df_to_correlate['darshan_normalized_perf_by_max'] < median_cutoff)\n",
    "    \n",
    "    # Calculate the correlation data for this vector\n",
    "    correlation = abcutils.correlation.calc_correlation_vector(\n",
    "        df_to_correlate[cutoff_filter],\n",
    "        correlate_with='darshan_normalized_perf_by_max')\n",
    "    \n",
    "    # rename the columns in this vector to include the file system name\n",
    "    new_cols = {}\n",
    "    for index, col_name in enumerate(correlation.columns):\n",
    "        new_cols[col_name] = \"%s %s\" % (fs, col_name)\n",
    "    correlation.rename(columns=new_cols, inplace=True)\n",
    "    \n",
    "    # join the vector to the previous vectors' dataframe\n",
    "    if correlations is None:\n",
    "        correlations = correlation\n",
    "    else:\n",
    "        correlations = pandas.concat([correlations, correlation], axis='columns')\n",
    "\n",
    "# Only draw metrics that show something interesting\n",
    "confidence_filter = None\n",
    "for fs in TEST_PLATFORMS:\n",
    "    subfilter = correlations['%s p-value' % fs] < 1.0e-5\n",
    "    subfilter = (correlations['%s coefficient' % fs] > 0.30) | (correlations['%s coefficient' % fs] < -0.30)\n",
    "    if confidence_filter is None:\n",
    "        confidence_filter = subfilter\n",
    "    else:\n",
    "        confidence_filter |= subfilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw the actual table\n",
    "ax = abcutils.plot.correlation_vector_table(\n",
    "    correlations[confidence_filter],\n",
    "    row_name_map=abcutils.CONFIG['metric_labels'])\n",
    "\n",
    "# Set the table width larger if displaying lots of metrics\n",
    "ax.get_figure().set_size_inches(24, 0.4 * len(correlations[confidence_filter]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show how different metrics correlate in different performance quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by = ['_test_platform', '_benchmark_id']\n",
    "summarized_metrics_grouped = df.groupby(by=group_by).describe()\n",
    "\n",
    "correlation_dfs = {}\n",
    "\n",
    "for quartile in '25%', '50%', '75%':\n",
    "    correlations = None\n",
    "    for fs in TEST_PLATFORMS:\n",
    "        # Start with a single file system worth of data\n",
    "        df_to_correlate = df[df['_test_platform'] == fs][INTERESTING_METRICS]\n",
    "\n",
    "        if quartile in ['25%', '75%']:\n",
    "            cutoff = summarized_metrics_grouped.loc[fs, benchmark_id]['darshan_normalized_perf_by_max'][quartile]\n",
    "            cutoff_filter = (df_to_correlate['darshan_normalized_perf_by_max'] < cutoff)\n",
    "        elif quartile == '50%': # actually the IQR\n",
    "            cutoff_low = summarized_metrics_grouped.loc[fs, benchmark_id]['darshan_normalized_perf_by_max']['25%']\n",
    "            cutoff_high = summarized_metrics_grouped.loc[fs, benchmark_id]['darshan_normalized_perf_by_max']['75%']\n",
    "            cutoff_filter = (df_to_correlate['darshan_normalized_perf_by_max'] > cutoff_low)\n",
    "            cutoff_filter &= (df_to_correlate['darshan_normalized_perf_by_max'] > cutoff_high)\n",
    "            \n",
    "        # Calculate the correlation data for this vector\n",
    "        correlation = abcutils.correlation.calc_correlation_vector(\n",
    "            df_to_correlate[cutoff_filter],\n",
    "            correlate_with='darshan_normalized_perf_by_max')\n",
    "\n",
    "        # rename the columns in this vector to include the file system name\n",
    "        new_cols = {}\n",
    "        for index, col_name in enumerate(correlation.columns):\n",
    "            new_cols[col_name] = \"%s %s\" % (fs, col_name)\n",
    "        correlation.rename(columns=new_cols, inplace=True)\n",
    "\n",
    "        # join the vector to the previous vectors' dataframe\n",
    "        if correlations is None:\n",
    "            correlations = correlation\n",
    "        else:\n",
    "            correlations = pandas.concat([correlations, correlation], axis='columns')\n",
    "    correlation_dfs[quartile] = correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for quartile in sorted(correlation_dfs.keys()):\n",
    "    correlations = correlation_dfs[quartile]\n",
    "    ax = abcutils.plot.correlation_vector_table(\n",
    "        correlations,\n",
    "        row_name_map=abcutils.CONFIG['metric_labels'])\n",
    "\n",
    "    # Set the table width larger if displaying lots of metrics\n",
    "    ax.get_figure().set_size_inches(20, 0.4 * len(correlations))\n",
    "    ax.set_title(quartile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_platform = 'scratch1@edison'\n",
    "\n",
    "for test_platform in sorted(df['_test_platform'].unique()):\n",
    "    fig, ax = matplotlib.pyplot.subplots()\n",
    "    fig.set_size_inches(16, 4)\n",
    "    ax.grid()\n",
    "    ax.set_ylim(-1.0, 1.0)\n",
    "    ax.set_ylabel(\"Correlation with Performance\")\n",
    "\n",
    "    width = 0.5\n",
    "\n",
    "    xticks = []\n",
    "    xticklabels = []\n",
    "    for index, plot_metric in enumerate(correlation_dfs['25%'].index):\n",
    "        xticks.append(float(index)*width*len(y))\n",
    "        xticklabels.append(plot_metric)\n",
    "        color = 'black' if index % 2 == 0 else 'gold'\n",
    "        \n",
    "        # plot each box in a cluster of three as an individual plt.box() call so\n",
    "        # that we can color the box edges independently and according to their\n",
    "        # p-value\n",
    "        for qindex, quartile in enumerate(['25%', '50%', '75%']):\n",
    "            pval = correlation_dfs[quartile][test_platform + \" p-value\"].loc[plot_metric]\n",
    "            if pval < 0.01:\n",
    "                edgecolor = 'blue'\n",
    "            elif pval < 0.05:\n",
    "                edgecolor = 'green'\n",
    "            else:\n",
    "                edgecolor = 'red'\n",
    "            ax.bar((qindex - 1) * width + xticks[-1],\n",
    "                   correlation_dfs[quartile][test_platform + \" coefficient\"].loc[plot_metric],\n",
    "                   width=width*0.95,\n",
    "                   color=color,\n",
    "                   edgecolor=edgecolor)\n",
    "\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels([abcutils.CONFIG['metric_labels'].get(x, x) for x in xticklabels], rotation=30, ha='right')\n",
    "    ax.set_title(test_platform)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
