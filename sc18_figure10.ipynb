{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import collections\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "import pandas\n",
    "import numpy\n",
    "import scipy.stats\n",
    "import abcutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Synthesize Data from CSV\n",
    "\n",
    "This process loads each summary CSV file, creates a few derived metrics, and then merges each system's CSV into a single global dataset that can be sliced and diced by system, benchmark, or any other way.  We are now caching the processed CSV in HDF5 format to speed up initial data ingest at the beginning of each analysis.  Delete the `CACHE_FILE` to re-generate this cache (e.g., when the contents of the CSV are updated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = abcutils.sc18paper.load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate a Single Test Platform\n",
    "\n",
    "Look at one combination of (compute system, file system, benchmark) to show what this UMAMI analysis can do.\n",
    "\n",
    "### Define Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric = 'darshan_normalized_perf_by_max'\n",
    "\n",
    "group_by = ['_test_platform', '_benchmark_id']\n",
    "\n",
    "print(\"plot_metric =\", abcutils.CONFIG['metric_labels'].get(plot_metric, plot_metric))\n",
    "print(\"date_start =\", abcutils.sc18paper.DATE_START.isoformat())\n",
    "print(\"date_end =\", abcutils.sc18paper.DATE_END.isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Width of simple moving average (SMA) short/long windows\n",
    "short_window = pandas.Timedelta(days=14)\n",
    "long_window = pandas.Timedelta(days=49)\n",
    "\n",
    "print(\"Short window will average over %s measurements at a time\" % short_window)\n",
    "print(\"Long window will average over %s measurements at a time\" % long_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually only attempt to correlate against a fixed subset of the features.  This is a time-saving measure; the features in `umami_row_order` were determined by performing an unguided correlation against everything and only selecting those features which showed some degree of correlation _and_ were not degenerate of other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRELATION_METRICS = abcutils.CONFIG['umami_row_order']\n",
    "CORRELATION_METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate or load calculated contributors\n",
    "\n",
    "This can take an inconvenient amount of time, so we cache the results to `contributors.hdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_contributors = None\n",
    "\n",
    "# Load the cached contributors list if available\n",
    "try:\n",
    "    all_contributors = pandas.read_hdf('contributors.hdf5', 'contributors')\n",
    "except IOError:\n",
    "    pass\n",
    "\n",
    "# Otherwise recalculate the contributors list\n",
    "if all_contributors is None:\n",
    "    grouped_df = filtered_df.groupby(by=group_by)\n",
    "    for group in grouped_df.groups:\n",
    "        example_df = grouped_df.get_group(group)\n",
    "\n",
    "        intercepts = abcutils.features.sma_intercepts(example_df,\n",
    "                                                      plot_metric,\n",
    "                                                      short_window=short_window,\n",
    "                                                      long_window=long_window)\n",
    "\n",
    "        loci = abcutils.features.generate_loci_sma(example_df,\n",
    "                                                   plot_metric,\n",
    "                                                   mins=True,\n",
    "                                                   maxes=False,\n",
    "                                                   short_window=short_window,\n",
    "                                                   long_window=long_window)\n",
    "        regions = list(abcutils.features.intercepts_to_region(example_df, intercepts))\n",
    "\n",
    "        for region in regions:\n",
    "            contributors = abcutils.classify.identify_contributors(region=region,\n",
    "                                                    target_column=plot_metric,\n",
    "                                                    target_index=region[plot_metric].idxmin(),\n",
    "                                                    correlate_columns=CORRELATION_METRICS,\n",
    "                                                    want_good=False,\n",
    "                                                    classifier='minmax')\n",
    "            if all_contributors is None:\n",
    "                all_contributors = contributors\n",
    "            else:\n",
    "                all_contributors = pandas.concat((all_contributors, contributors))\n",
    "\n",
    "    # Cache the contributors list for the next time\n",
    "    all_contributors.index = numpy.arange(len(all_contributors))\n",
    "    all_contributors.to_hdf('contributors.hdf5', key='contributors', mode='w', format='fixed', complevel=9, complib='zlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of target indices:\", len(all_contributors['target_index'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply filters to remove very high p-value measurements from the dataset.  These cause problems when performing significance testing later on, since they dilute the significance of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also drop any contributors who lack statistical confidence because of duplicate values\n",
    "significant_contributors = all_contributors[all_contributors['pvalue'] < 0.10]\n",
    "print(\"Discarding %d contributors with p-values < 0.10\" % (len(all_contributors) - len(significant_contributors)))\n",
    "\n",
    "# Keep all data and let p-values speak for themselves\n",
    "#significant_contributors = all_contributors\n",
    "\n",
    "print(\"Number of contributors remaining:\", len(significant_contributors))\n",
    "print(\"Number of target indices ('bad' jobs):\", len(significant_contributors['target_index'].unique()))\n",
    "print(\"Number of unclassified jobs:\", (all_contributors.groupby(['target_index']).sum()['target_metric_matches'] < 1.0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table is the total number of observations broken down by file system.  For example, `fs_ave_mds_cpu` for `cscratch@cori-knl` = `102.0` means that there were 102 cases where poor performance was observed on `cscratch@cori-knl` while at the same time the `fs_ave_mds_cpu` metric was available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_observation_counts = pandas.pivot_table(significant_contributors,\n",
    "                                               values='target_metric_matches',\n",
    "                                               index=['metric_name'],\n",
    "                                               columns=['_test_platform'],\n",
    "                                               aggfunc=lambda x: (~numpy.isnan(x)).sum()).fillna(0.0)\n",
    "metric_observation_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows how many of the observations in the previous table were actually implicated (tagged) as being correlated with poor performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_metric_counts = pandas.pivot_table(significant_contributors,\n",
    "                                          values='target_metric_matches',\n",
    "                                          index=['metric_name'],\n",
    "                                          columns=['_test_platform'],\n",
    "                                          aggfunc=numpy.sum).fillna(0.0)\n",
    "tagged_metric_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then divide the number of times each metric was implicated (tagged) as correlating with poor performance by the total number of times that metric was observed on each file system.  The result is the fraction of times each metric was observed to correlate with poor performance on a per-file system basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributor_distribution = tagged_metric_counts.div(metric_observation_counts.sum(axis=1), axis=0)\n",
    "contributor_distribution = contributor_distribution.reindex(index=contributor_distribution.sum(axis=1).sort_values(ascending=False).index)\n",
    "try:\n",
    "    del contributor_distribution.columns.name\n",
    "except AttributeError:\n",
    "    pass\n",
    "contributor_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then take the total number of times each metric was tagged across _all file systems_ (across an entire row in the previous table) and divide it by the total number of observations of that metric to calculate the fraction of observations where each metric was tagged as being correlated with poor performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_stacked_bars(contributor_distribution, legendprops=None):\n",
    "    _legendprops = {}\n",
    "    if legendprops:\n",
    "        _legendprops.update(legendprops)\n",
    "    \n",
    "    row_sums = contributor_distribution.sum(axis=1)\n",
    "\n",
    "    fig, ax = matplotlib.pyplot.subplots(figsize=(8,4))\n",
    "\n",
    "    contributor_distribution.plot.bar(stacked=True, ax=ax, width=0.90)\n",
    "    ax.grid()\n",
    "    ax.set_ylim(0, 0.5)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "    xticklabels = [abcutils.CONFIG['umami_rows'].get(x.get_text(), x.get_text()) for x in ax.get_xticklabels()]\n",
    "    ax.set_xticklabels(xticklabels, rotation=30, ha='right')\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Fraction of tests\")\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [abcutils.CONFIG['platform_labels'].get(x, x) for x in labels], **_legendprops)\n",
    "\n",
    "    for index, x_value in enumerate(ax.get_xticks()):\n",
    "        ax.annotate(\"%d%%\" % (100.0 * row_sums[index]), xy=(x_value, row_sums[index] + 0.02),\n",
    "                    ha='center',\n",
    "                    backgroundcolor='#FFFFFFAA')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assert confidence\n",
    "\n",
    "We use the binomial test to calculate the p-values of each fraction of tests asserted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for metric in contributor_distribution.index.values: # ['fs_ave_oss_cpu']: #\n",
    "    prob_success = 1.0\n",
    "    prob_failure = 1.0\n",
    "\n",
    "    successes = []\n",
    "    failures = []\n",
    "\n",
    "    num_matches = 0\n",
    "    metric_measures = significant_contributors[significant_contributors['metric_name'] == metric]\n",
    "    \n",
    "    for row in metric_measures.itertuples():\n",
    "        if row.target_metric_matches:\n",
    "            num_matches += 1\n",
    "            prob_success *= row.pvalue\n",
    "            successes.append(row.pvalue)\n",
    "        else:\n",
    "            prob_failure *= (1.0 - row.pvalue)\n",
    "            failures.append(row.pvalue)\n",
    "\n",
    "    pick_n = num_matches\n",
    "    out_of = len(metric_measures)\n",
    "    \n",
    "    if not successes:\n",
    "        continue\n",
    "\n",
    "    # what is the probability that we observe pick_n / out_of jobs with this\n",
    "    # tagged metric given the probability of encountering a tagged metric\n",
    "    # if there's no relationship between this metric being tagged and each\n",
    "    # job's performance?\n",
    "    #\n",
    "    # binomial test: assume the null hypothesis is TRUE\n",
    "    #   1. pick the highest p-value observed for this metric - that is the\n",
    "    #      case where the null hypothesis is most likely to be true\n",
    "    #   2. perform the binomial test to see what the odds are of observing\n",
    "    #      pick_n **or more** tagged metrics if the null hypothesis is true?\n",
    "    probability = numpy.max(successes)\n",
    "    pvalue = scipy.stats.binom_test(pick_n,\n",
    "                                    out_of,\n",
    "                                    probability,\n",
    "                                    alternative='greater')\n",
    "    \n",
    "    result = collections.OrderedDict({})\n",
    "    result['metric'] = metric\n",
    "    result['pick_n'] = pick_n\n",
    "    result['out_of'] = out_of\n",
    "    result['probability_used'] = probability\n",
    "    result['calculated_pvalue'] = pvalue\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "binomial_results = pandas.DataFrame.from_dict(results).set_index('metric')\n",
    "binomial_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = draw_stacked_bars(contributor_distribution.loc[binomial_results.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shade off the statistically insignificant metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a mapping from metrics to rectangles\n",
    "reverse_metric_map = {}\n",
    "for key, val in abcutils.CONFIG['umami_rows'].items():\n",
    "    reverse_metric_map[val] = key\n",
    "    if 'CF' in val:\n",
    "        reverse_metric_map[val.replace(' CF', ' Contention')] = key\n",
    "\n",
    "# Find all rectangles corresponding to each metric\n",
    "rectangle_map = {}\n",
    "xticks = ax.get_xticks()\n",
    "xticklabels = [x.get_text() for x in ax.get_xticklabels()]\n",
    "for child in ax.get_children():\n",
    "    if isinstance(child, matplotlib.patches.Rectangle) and child.get_width() == 0.9:\n",
    "        child_x = int(round(child.xy[0] + child.get_width() / 2))\n",
    "        key = reverse_metric_map[xticklabels[child_x]]\n",
    "        if key not in rectangle_map:\n",
    "            rectangle_map[key] = []\n",
    "        rectangle_map[key].append(child)\n",
    "\n",
    "# Actually apply a grey box over the box of each metric that is not statistically significant\n",
    "for row in binomial_results.itertuples():\n",
    "    if row.calculated_pvalue > 0.10:\n",
    "        for rectangle in rectangle_map[row.Index]:\n",
    "            rectangle.set_color(\"#DDDDDD\")\n",
    "            rectangle.set_edgecolor('#DDDDDD')\n",
    "\n",
    "ax.xaxis.grid(False)\n",
    "ax.get_figure().savefig('figs/contributors-bad-by-system-grey.pdf', bbox_inches='tight')\n",
    "ax.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also provide a less confusing version of the plot without the per-file system resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in binomial_results.itertuples():\n",
    "    if row.calculated_pvalue > 0.10:\n",
    "        for rectangle in rectangle_map[row.Index]:\n",
    "            rectangle.set_color(\"#DDDDDD\")\n",
    "            rectangle.set_edgecolor('#DDDDDD')\n",
    "    else:\n",
    "        for rectangle in rectangle_map[row.Index]:\n",
    "            rectangle.set_color(\"C0\")\n",
    "            rectangle.set_edgecolor('C0')\n",
    "ax.get_legend().set_visible(False)\n",
    "ax.get_figure().savefig('figs/contributors-bad-grey.pdf', bbox_inches='tight')\n",
    "ax.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then only show the metrics that are statistically significant at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = draw_stacked_bars(contributor_distribution.loc[(binomial_results['calculated_pvalue'] < 0.10).values],\n",
    "                      legendprops={\n",
    "                          'loc': 'upper right',\n",
    "                          'bbox_to_anchor': (1.01, 1.03),\n",
    "                          'labelspacing': 0.4\n",
    "                      })\n",
    "ax.set_xticklabels([x.get_text().replace(' CF', '\\nContention') for x in ax.get_xticklabels()], rotation=30)\n",
    "ax.xaxis.grid(False)\n",
    "ax.get_figure().savefig('figs/contributors-bad-by-system.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_results.sort_values('calculated_pvalue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
