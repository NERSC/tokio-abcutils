{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import collections\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "import pandas\n",
    "import numpy\n",
    "import scipy.stats\n",
    "import abcutils\n",
    "\n",
    "numpy.random.seed(int(time.mktime(datetime.datetime.now().timetuple())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Synthesize Data from CSV\n",
    "\n",
    "This process loads each summary CSV file, creates a few derived metrics, and then merges each system's CSV into a single global dataset that can be sliced and diced by system, benchmark, or any other way.  We are now caching the processed CSV in HDF5 format to speed up initial data ingest at the beginning of each analysis.  Delete the `CACHE_FILE` to re-generate this cache (e.g., when the contents of the CSV are updated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_df = abcutils.sc18paper.load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate a Single Test Platform\n",
    "\n",
    "Look at one combination of (compute system, file system, benchmark) to show what this UMAMI analysis can do.\n",
    "\n",
    "### Define Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_metric = 'darshan_normalized_perf_by_max'\n",
    "\n",
    "group_by = ['_test_platform', '_benchmark_id']\n",
    "\n",
    "print \"plot_metric =\", abcutils.CONFIG['metric_labels'].get(plot_metric, plot_metric)\n",
    "print \"date_start =\", abcutils.sc18paper.DATE_START.isoformat()\n",
    "print \"date_end =\", abcutils.sc18paper.DATE_END.isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Width of simple moving average (SMA) short/long windows\n",
    "short_window = pandas.Timedelta(days=14)\n",
    "long_window = pandas.Timedelta(days=49)\n",
    "\n",
    "print \"Short window will average over %s measurements at a time\" % short_window\n",
    "print \"Long window will average over %s measurements at a time\" % long_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate or load calculated contributors\n",
    "\n",
    "This can take an inconvenient amount of time, so we cache the results to `contributors.hdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_contributors = None\n",
    "\n",
    "try:\n",
    "    all_contributors = pandas.read_hdf('contributors.hdf5', 'contributors')\n",
    "except IOError:\n",
    "    pass\n",
    "\n",
    "if all_contributors is None:\n",
    "    grouped_df = filtered_df.groupby(by=group_by)\n",
    "    for group in grouped_df.groups:\n",
    "        example_df = grouped_df.get_group(group)\n",
    "\n",
    "        intercepts = abcutils.features.sma_intercepts(example_df,\n",
    "                                                      plot_metric,\n",
    "                                                      short_window=short_window,\n",
    "                                                      long_window=long_window)\n",
    "\n",
    "        loci = abcutils.features.generate_loci_sma(example_df,\n",
    "                                                   plot_metric,\n",
    "                                                   mins=True,\n",
    "                                                   maxes=False,\n",
    "                                                   short_window=short_window,\n",
    "                                                   long_window=long_window)\n",
    "        regions = list(abcutils.features.intercepts_to_region(example_df, intercepts))\n",
    "\n",
    "        for region in regions:\n",
    "            contributors = abcutils.classify.identify_contributors(region=region,\n",
    "                                                    target_column=plot_metric,\n",
    "                                                    target_index=region[plot_metric].idxmin(),\n",
    "                                                    correlate_columns=abcutils.CONFIG['umami_row_order'],\n",
    "                                                    want_good=False,\n",
    "                                                    classifier='minmax')\n",
    "            if all_contributors is None:\n",
    "                all_contributors = contributors\n",
    "            else:\n",
    "                all_contributors = pandas.concat((all_contributors, contributors))\n",
    "\n",
    "    all_contributors.index = numpy.arange(len(all_contributors))\n",
    "    all_contributors.to_hdf('contributors.hdf5', key='contributors', mode='w', format='fixed', complevel=9, complib='zlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Number of target indices:\", len(all_contributors['target_index'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply filters to remove very high p-value measurements from the dataset.  These cause problems when performing significance testing later on, since they dilute the significance of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop any contributors who lack statistical confidence by virtue of their domain\n",
    "#significant_contributors = all_contributors[all_contributors['random_pvalue'] < 0.10]\n",
    "\n",
    "# Also drop any contributors who lack statistical confidence because of duplicate values\n",
    "significant_contributors = all_contributors[all_contributors['pvalue'] < 0.10]\n",
    "\n",
    "# Keep all data and let p-values speak for themselves\n",
    "#significant_contributors = all_contributors\n",
    "\n",
    "print \"Discarding %d contributors with p-values < 0.10\" % (len(all_contributors) - len(significant_contributors))\n",
    "\n",
    "print \"Number of contributors remaining:\", len(significant_contributors)\n",
    "print \"Number of target indices ('bad' jobs):\", len(significant_contributors['target_index'].unique())\n",
    "print \"Number of unclassified jobs:\", (all_contributors.groupby(['target_index']).sum()['target_metric_matches'] < 1.0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = matplotlib.pyplot.subplots(figsize=(8,4))\n",
    "\n",
    "print \"Average fraction of matches per metric\"\n",
    "significant_contributors[['_test_platform', 'target_metric_matches', 'metric_name']]\\\n",
    "    .groupby(['metric_name'])\\\n",
    "    .mean()\\\n",
    "    .plot(kind='bar', ax=ax)\n",
    "ax.get_legend().set_visible(False)\n",
    "ax.set_ylabel(\"Fraction of bad jobs\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha='right')\n",
    "ax.yaxis.grid(True)\n",
    "ax.set_axisbelow(True)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tagged_metric_counts = pandas.pivot_table(significant_contributors,\n",
    "                                          values='target_metric_matches',\n",
    "                                          index=['metric_name'],\n",
    "                                          columns=['_test_platform'],\n",
    "                                          aggfunc=numpy.sum).fillna(0.0)\n",
    "tagged_metric_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_observation_counts = pandas.pivot_table(significant_contributors,\n",
    "                                               values='target_metric_matches',\n",
    "                                               index=['metric_name'],\n",
    "                                               columns=['_test_platform'],\n",
    "                                               aggfunc=lambda x: (~numpy.isnan(x)).sum()).fillna(0.0)\n",
    "metric_observation_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the number of contributors by the number of times that contributor was\n",
    "# ever observed to get its contribution to the overall fraction of regions where\n",
    "# that metric was implicated\n",
    "contributor_distribution = tagged_metric_counts.div(metric_observation_counts.sum(axis=1), axis=0)\n",
    "contributor_distribution = contributor_distribution.reindex(index=contributor_distribution.sum(axis=1).sort_values(ascending=False).index)\n",
    "try:\n",
    "    del contributor_distribution.columns.name\n",
    "except AttributeError:\n",
    "    pass\n",
    "contributor_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_stacked_bars(contributor_distribution, legendprops=None):\n",
    "    _legendprops = {}\n",
    "    if legendprops:\n",
    "        _legendprops.update(legendprops)\n",
    "    \n",
    "    row_sums = contributor_distribution.sum(axis=1)\n",
    "\n",
    "    fig, ax = matplotlib.pyplot.subplots(figsize=(8,4))\n",
    "\n",
    "    contributor_distribution.plot.bar(stacked=True, ax=ax, width=0.90)\n",
    "    ax.grid()\n",
    "    ax.set_ylim(0, 0.5)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "    xticklabels = [abcutils.CONFIG['umami_rows'].get(x.get_text()) for x in ax.get_xticklabels()]\n",
    "    ax.set_xticklabels(xticklabels, rotation=30, ha='right')\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Fraction of tests\")\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [abcutils.CONFIG['platform_labels'].get(x, x) for x in labels], **_legendprops)\n",
    "\n",
    "    for index, x_value in enumerate(ax.get_xticks()):\n",
    "        ax.annotate(\"%d%%\" % (100.0 * row_sums[index]), xy=(x_value, row_sums[index] + 0.02),\n",
    "                    ha='center',\n",
    "                    backgroundcolor='#FFFFFFAA')\n",
    "\n",
    "    return ax\n",
    "\n",
    "draw_stacked_bars(contributor_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assert confidence\n",
    "\n",
    "We use the binomial test to calculate the p-values of each fraction of tests asserted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for metric in contributor_distribution.index.values: # ['fs_ave_oss_cpu']: #\n",
    "    prob_success = 1.0\n",
    "    prob_failure = 1.0\n",
    "\n",
    "    successes = []\n",
    "    failures = []\n",
    "\n",
    "    num_matches = 0\n",
    "    metric_measures = significant_contributors[significant_contributors['metric_name'] == metric]\n",
    "    \n",
    "    for row in metric_measures.itertuples():\n",
    "        if row.target_metric_matches:\n",
    "            num_matches += 1\n",
    "            prob_success *= row.pvalue\n",
    "            successes.append(row.pvalue)\n",
    "        else:\n",
    "            prob_failure *= (1.0 - row.pvalue)\n",
    "            failures.append(row.pvalue)\n",
    "\n",
    "    pick_n = num_matches\n",
    "    out_of = len(metric_measures)\n",
    "    \n",
    "    if not successes:\n",
    "        continue\n",
    "\n",
    "    # what is the probability that we observe pick_n / out_of jobs with this\n",
    "    # tagged metric given the probability of encountering a tagged metric\n",
    "    # if there's no relationship between this metric being tagged and each\n",
    "    # job's performance?\n",
    "    #\n",
    "    # binomial test: assume the null hypothesis is TRUE\n",
    "    #   1. pick the highest p-value observed for this metric - that is the\n",
    "    #      case where the null hypothesis is most likely to be true\n",
    "    #   2. perform the binomial test to see what the odds are of observing\n",
    "    #      pick_n **or more** tagged metrics if the null hypothesis is true?\n",
    "    probability = numpy.max(successes)\n",
    "    pvalue = scipy.stats.binom_test(pick_n,\n",
    "                                    out_of,\n",
    "                                    probability,\n",
    "                                    alternative='greater')\n",
    "    \n",
    "    result = collections.OrderedDict({})\n",
    "    result['metric'] = metric\n",
    "    result['pick_n'] = pick_n\n",
    "    result['out_of'] = out_of\n",
    "    result['probability_used'] = probability\n",
    "    result['calculated_pvalue'] = pvalue\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "binomial_results = pandas.DataFrame.from_dict(results).set_index('metric')\n",
    "binomial_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = draw_stacked_bars(contributor_distribution.loc[binomial_results.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shade off the statistically insignificant metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a mapping from metrics to rectangles\n",
    "reverse_metric_map = {}\n",
    "for key, val in abcutils.CONFIG['umami_rows'].iteritems():\n",
    "    reverse_metric_map[val] = key\n",
    "\n",
    "rectangle_map = {}\n",
    "xticks = ax.get_xticks()\n",
    "xticklabels = [x.get_text() for x in ax.get_xticklabels()]\n",
    "for child in ax.get_children():\n",
    "    if isinstance(child, matplotlib.patches.Rectangle) and child.get_width() == 0.9:\n",
    "        child_x = int(round(child.xy[0] + child.get_width() / 2))\n",
    "        key = reverse_metric_map[xticklabels[child_x]]\n",
    "        if key not in rectangle_map:\n",
    "            rectangle_map[key] = []\n",
    "        rectangle_map[key].append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in binomial_results.itertuples():\n",
    "    if row.calculated_pvalue > 0.10:\n",
    "        for rectangle in rectangle_map[row.Index]:\n",
    "            rectangle.set_color(\"#DDDDDD\")\n",
    "            rectangle.set_edgecolor('#DDDDDD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.xaxis.grid(False)\n",
    "ax.get_figure().savefig('figs/contributors-bad-by-system-grey.pdf', bbox_inches='tight')\n",
    "ax.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in binomial_results.itertuples():\n",
    "    if row.calculated_pvalue > 0.10:\n",
    "        for rectangle in rectangle_map[row.Index]:\n",
    "            rectangle.set_color(\"#DDDDDD\")\n",
    "            rectangle.set_edgecolor('#DDDDDD')\n",
    "    else:\n",
    "        for rectangle in rectangle_map[row.Index]:\n",
    "            rectangle.set_color(\"C0\")\n",
    "            rectangle.set_edgecolor('C0')\n",
    "ax.get_legend().set_visible(False)\n",
    "ax.get_figure().savefig('figs/contributors-bad-grey.pdf', bbox_inches='tight')\n",
    "ax.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trim off all metrics which are not statistically significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = draw_stacked_bars(contributor_distribution.loc[(binomial_results['calculated_pvalue'] < 0.10).values],\n",
    "                      legendprops={\n",
    "                          'loc': 'upper right',\n",
    "                          'bbox_to_anchor': (1.01, 1.03),\n",
    "                          'labelspacing': 0.4\n",
    "                      })\n",
    "ax.set_xticklabels([x.get_text().replace(' CF', '\\nCoverage Factor') for x in ax.get_xticklabels()], rotation=30)\n",
    "ax.xaxis.grid(False)\n",
    "ax.get_figure().savefig('figs/contributors-bad-by-system.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_results.sort_values('calculated_pvalue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
